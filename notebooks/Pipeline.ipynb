{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "architectural-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib.pyplot import figure\n",
    "from sodapy import Socrata\n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, KBinsDiscretizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "coastal-challenge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aequitas.group import Group\n",
    "from aequitas.bias import Bias\n",
    "from aequitas.fairness import Fairness\n",
    "from aequitas.plotting import Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*Conexión a la API Chicago Food Insp*\n",
    "#client = Socrata(\"data.cityofchicago.org\", None)\n",
    "#results = client.get(\"4ijn-s7e5\", limit=400000)\n",
    "#df = pd.DataFrame.from_records(results)\n",
    "#col_names = df.columns.to_list()\n",
    "#col_name = []\n",
    "#for i in range(len(col_names)):\n",
    "#    col_name.append(col_names[i].replace(\" \", \"_\").lower())\n",
    "#df.columns =col_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-garlic",
   "metadata": {},
   "source": [
    "## Limpieza y preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sensitive-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarize_column_strings(df, columns, excluded_punctuation=\".,*¿?¡!\"):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(\" \", \"_\")\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(\"-\", \"_\")\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(\"á\", \"a\")\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(\"é\", \"e\")\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(\"í\", \"i\")\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(\"ó\", \"o\")\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(\"ú\", \"u\")\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(\"ü\", \"u\")\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(r\"[^a-zA-Z\\d\\_]+\", \"\")\n",
    "        for ch in excluded_punctuation:\n",
    "            df[col] = df[col].str.replace(ch, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sorted-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(df):\n",
    "    '''\n",
    "    Función que convierte las columnas del Data Frame al tipo y forma que se necesita para\n",
    "    los análisis posteriores\n",
    "    \n",
    "    inputs: Data Frame almacenado en el S3 (ingesta.pkl)\n",
    "    outputs: Data Frame con las variables en formato adecuado (df_clean.pkl)\n",
    "        \n",
    "    '''\n",
    "    #df = pickle.load(open(\"ingesta.pkl\",\"rb\"))\n",
    "    nrows_prev = df.shape[0]\n",
    "    ncols_prev = df.shape[1]\n",
    "    data_null_prev = df.isnull().sum().sum()\n",
    "    # Variables de texto\n",
    "    df['violations']= df['violations'].astype('object')\n",
    "    df['violations_count'] = df.violations.str.count(r'\\|')+1\n",
    "    df['violations_count'] = df.violations_count.fillna(0)\n",
    "    df['violations_count'] = df['violations_count'].astype('int')\n",
    "    # Variables categóricas\n",
    "    df['dba_name']= df['dba_name'].astype('object')\n",
    "    df['aka_name']= df['aka_name'].astype('object')\n",
    "    df['facility_type']= df['facility_type'].astype('category')\n",
    "    df['risk']= df['risk'].astype('category')\n",
    "    df['address']= df['address'].astype('category')\n",
    "    df['city']= df['city'].astype('category')\n",
    "    df['state']= df['state'].astype('category')\n",
    "    df['inspection_type']= df['inspection_type'].astype('category')\n",
    "    df['results']= df['results'].astype('category')\n",
    "    # Variable label_risk\n",
    "    df['risk'] = df['risk'].replace([\"Risk 1 (High)\"],3)\n",
    "    df['risk'] = df['risk'].replace([\"Risk 2 (Medium)\"],2)\n",
    "    df['risk'] = df['risk'].replace([\"Risk 3 (Low)\"],1)\n",
    "    df['risk'] = df['risk'].replace([\"All\"],0)\n",
    "    df['risk'] = pd.to_numeric(df['risk'], errors='coerce')\n",
    "    df=df.rename(columns = {'risk':'label_risk'})\n",
    "    df['label_risk'] = df['label_risk'].fillna(3)\n",
    "    df['label_risk'] = df['label_risk'].astype('int')\n",
    "    # Variables de fecha\n",
    "    df['inspection_date'] = pd.to_datetime(df['inspection_date'], infer_datetime_format=True)\n",
    "    df['inspection_month']=df['inspection_date'].dt.month\n",
    "    MONTH = 12\n",
    "    df['sin_mnth'] = np.sin(2*np.pi*df.inspection_month/MONTH)\n",
    "    df['cos_mnth'] = np.cos(2*np.pi*df.inspection_month/MONTH)\n",
    "    df['inspection_weekday']=df['inspection_date'].dt.weekday\n",
    "    WEEKDAY = 7\n",
    "    df['sin_wkd'] = np.sin(2*np.pi*df.inspection_weekday/WEEKDAY)\n",
    "    df['cos_wkd'] = np.cos(2*np.pi*df.inspection_weekday/WEEKDAY)\n",
    "    # Etiqueta\n",
    "    df['label_results'] = df['results'].apply(lambda x: int(0) if x == 'Fail' else (int(1) if x in ['Pass','Pass w/Conditions'] else int(2)))\n",
    "    # Imputación de datos\n",
    "    df.drop(['violations'],axis = 1, inplace = True)\n",
    "    df.drop(['results'], axis = 1, inplace = True)\n",
    "    df.drop(df.loc[df['license_'].isnull()].index, inplace=True)\n",
    "    df.drop(df.loc[df['zip'].isnull()].index, inplace=True)\n",
    "    df.drop(df.loc[df['label_results'] == 2].index, inplace=True)\n",
    "    df['aka_name'] = df['aka_name'].fillna(df['dba_name'])\n",
    "    df['dba_name']= df['dba_name'].astype(str).str.lower()\n",
    "    df['aka_name']= df['aka_name'].astype(str).str.lower()\n",
    "    df['facility_type']= df['facility_type'].astype(str).str.lower()\n",
    "    df['state']= df['state'].astype(str).str.lower()\n",
    "    df['inspection_type']= df['inspection_type'].astype(str).str.lower()\n",
    "    df = df[~df['state'].isin(['wi', 'ny', 'in'])]\n",
    "    col_text = ['dba_name','aka_name']\n",
    "    df.rename(columns={'license_':'license'}, inplace=True)\n",
    "    standarize_column_strings(df, col_text)\n",
    "    df_dict_dummy = pd.DataFrame(df['aka_name'])\n",
    "    df_dict_dummy['facility_type'] = df['facility_type']\n",
    "    df_dict_dummy.drop(df_dict_dummy.loc[df_dict_dummy['facility_type'].isnull()].index, inplace=True)\n",
    "    group = df_dict_dummy.groupby('aka_name')\n",
    "    df_dict_dummy2 = group.apply(lambda x: x['facility_type'].unique())\n",
    "    df_dict_dummy3 = df_dict_dummy2.to_frame()\n",
    "    df_dict_dummy3.reset_index(level = 'aka_name', inplace = True)\n",
    "    df_dict_dummy3 = df_dict_dummy3.rename(columns = {0:'facility_type'})\n",
    "    df_dict_dummy3['facility_type'] = df_dict_dummy3['facility_type'].apply(lambda x: str(x[0]))\n",
    "    df2 = pd.merge(df,df_dict_dummy3, how = 'left', on = 'aka_name')\n",
    "    df2['facility_type_x'] = df2['facility_type_x'].fillna(df2['facility_type_y'])\n",
    "    df2['facility_type_x'] = df2['facility_type_x'].fillna('restaurant')\n",
    "    df2=df2.rename(columns = {'facility_type_x':'facility_type'})\n",
    "    df2.drop(['inspection_id','dba_name','address','city','state','latitude','longitude','location','facility_type_y','inspection_weekday','inspection_month'],axis = 1, inplace = True)\n",
    "    df2 = df2.dropna() #---------------------------------------->adición\n",
    "    nrows_after = df2.shape[0]\n",
    "    ncols_after = df2.shape[1]\n",
    "    return df2, nrows_prev, ncols_prev, nrows_after, ncols_after, data_null_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-practitioner",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "experimental-merit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zips(x,lev,dic):\n",
    "    if x in lev.zip.to_list():\n",
    "        return dic[x]\n",
    "    else:\n",
    "        return 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "comparative-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_eng2(df_fe):\n",
    "    '''\n",
    "    Función que realiza la selección de los features que serán utilizdos para la clasificación\n",
    "    \n",
    "    inputs: Data Frame limpio (df_clean.pkl)\n",
    "    outputs: Data Frame con la matriz de diseño para el modelo (df_clean.pkl)\n",
    "        \n",
    "    '''\n",
    "    # Transformación de variables facility_type y zip\n",
    "    tipo = pd.DataFrame(df_fe.facility_type.value_counts())\n",
    "    tipo['name'] = tipo.index\n",
    "    tipo.index = range(len(tipo.name))\n",
    "    grupo1 = tipo.iloc[0:4,1].tolist()\n",
    "    grupo2 = tipo.iloc[[5,6,7,11],1].tolist()\n",
    "    df_fe['class'] = df_fe['facility_type'].apply(lambda x: x if x in grupo1 else ('daycare' if x in grupo2 else 'other'))\n",
    "    lev = pd.read_csv('levels.csv')\n",
    "    lev['zip'] = lev['zip'].astype(str)\n",
    "    lev.index = lev.zip\n",
    "    dic = lev.level.to_dict()\n",
    "    df_fe['level'] = df_fe['zip'].apply(lambda x: zips(x,lev,dic))\n",
    "    # Transformación a OHE\n",
    "    df_fe = df_fe.sort_values(by='inspection_date', ascending=True)\n",
    "    df_input = pd.DataFrame(df_fe[['label_risk','label_results','level','class']])\n",
    "    data_input_ohe = pd.get_dummies(df_input)\n",
    "    etiqueta = data_input_ohe.label_results\n",
    "    data_input_ohe= data_input_ohe.drop('label_results', axis = 1)\n",
    "    variables_lista = list(data_input_ohe.columns)\n",
    "    # Grid Search\n",
    "    np.random.seed(20201124)\n",
    "    # ocuparemos un RF\n",
    "    classifier = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=1234)\n",
    "    # separando en train, test\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(data_input_ohe, etiqueta, test_size=0.3)\n",
    "\n",
    "    # definicion de los hiperparametros que queremos probar\n",
    "    hyper_param_grid = {'n_estimators': [300, 400], #'min_samples_leaf': [3,5,7],\n",
    "                        'max_depth': [7, 10],\n",
    "                        #'min_samples_split': [3],\n",
    "                        'max_features': [5, 10],\n",
    "                        'criterion': ['gini']}\n",
    "    # usamos TimeSeriesSplit para dividir respetando el orden cronológico\n",
    "    tscv = TimeSeriesSplit(n_splits=6)\n",
    "    # This was the trickiest part as a newbie. Straight from the docs\n",
    "    # If you only have experience with CV splits this way\n",
    "    # of making the splits might seem foreign. Fret not.\n",
    "    for train_index, test_index in tscv.split(data_input_ohe):\n",
    "        X_train, X_test = data_input_ohe.iloc[train_index, :], data_input_ohe.iloc[test_index,:]\n",
    "        y_train, y_test = etiqueta.iloc[train_index], etiqueta.iloc[test_index]\n",
    "    # ocupemos grid search\n",
    "    gs = GridSearchCV(classifier, \n",
    "                           hyper_param_grid, \n",
    "                           scoring = 'precision', return_train_score=True,\n",
    "                           cv = tscv)\n",
    "    start_time = time.time()\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_rf = gs.best_estimator_\n",
    "    best_score = gs.best_estimator_.oob_score_\n",
    "    feature_importance = pd.DataFrame({'importance':\\\n",
    "                                       best_rf.feature_importances_,\\\n",
    "                                       'feature': variables_lista})\n",
    "    feature_importance=feature_importance.sort_values(by=\"importance\", ascending=False)\n",
    "    #fi_out = feature_importance.head(10)\n",
    "    time_exec = time.time() - start_time\n",
    "    nrows_ohe = data_input_ohe.shape[0]\n",
    "    ncols_ohe = data_input_ohe.shape[1]\n",
    "    #print(\"Tiempo en ejecutar: \", time.time() - start_time)\n",
    "    return df_input, nrows_ohe, ncols_ohe, float(best_score), time_exec, str(best_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "suffering-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_eng(df_fe):\n",
    "    '''\n",
    "    Función que realiza la selección de los features que serán utilizdos para la clasificación\n",
    "    \n",
    "    inputs: Data Frame limpio (df_clean.pkl)\n",
    "    outputs: Data Frame con la matriz de diseño para el modelo (df_clean.pkl)\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Transformación de variables facility_type y zip\n",
    "    tipo = pd.DataFrame(df_fe.facility_type.value_counts())\n",
    "    tipo['name'] = tipo.index\n",
    "    tipo.index = range(len(tipo.name))\n",
    "    grupo1 = tipo.iloc[0:4,1].tolist()\n",
    "    grupo2 = tipo.iloc[[5,6,7,11],1].tolist()\n",
    "    df_fe['class'] = df_fe['facility_type'].apply(lambda x: x if x in grupo1 else ('daycare' if x in grupo2 else 'other'))\n",
    "    lev = pd.read_csv('zip_catalog.csv')\n",
    "    lev['zip'] = lev['zip'].astype(str)\n",
    "    lev.index = lev.zip\n",
    "    dic = lev.level.to_dict()\n",
    "    df_fe['level'] = df_fe['zip'].apply(lambda x: zips(x,lev,dic))\n",
    "    \n",
    "    \n",
    "    # Transformación a OHE\n",
    "    df_fe = df_fe.sort_values(by='inspection_date', ascending=True)\n",
    "    df_input = pd.DataFrame(df_fe[['label_risk','label_results','level','class']])\n",
    "    data_input_ohe = pd.get_dummies(df_input)\n",
    "    etiqueta = data_input_ohe.label_results\n",
    "    data_input_ohe= data_input_ohe.drop('label_results', axis = 1)\n",
    "    variables_lista = list(data_input_ohe.columns)\n",
    "    # Grid Search\n",
    "    np.random.seed(20201124)\n",
    "    # ocuparemos un RF\n",
    "    classifier = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=1234)\n",
    "    # separando en train, test\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(data_input_ohe, etiqueta, test_size=0.3)\n",
    "\n",
    "    # definicion de los hiperparametros que queremos probar\n",
    "    hyper_param_grid = {'n_estimators': [300, 400], #'min_samples_leaf': [3,5,7],\n",
    "                        'max_depth': [7, 10],\n",
    "                        #'min_samples_split': [3],\n",
    "                        'max_features': [3, 5, 6],\n",
    "                        'criterion': ['gini']}\n",
    "    # usamos TimeSeriesSplit para dividir respetando el orden cronológico\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    # This was the trickiest part as a newbie. Straight from the docs\n",
    "    # If you only have experience with CV splits this way\n",
    "    # of making the splits might seem foreign. Fret not.\n",
    "    for train_index, test_index in tscv.split(data_input_ohe):\n",
    "        X_train, X_test = data_input_ohe.iloc[train_index, :], data_input_ohe.iloc[test_index,:]\n",
    "        y_train, y_test = etiqueta.iloc[train_index], etiqueta.iloc[test_index]\n",
    "    # ocupemos grid search\n",
    "    gs = GridSearchCV(classifier, \n",
    "                           hyper_param_grid, \n",
    "                           scoring = 'precision', return_train_score=True,\n",
    "                           cv = tscv)\n",
    "    start_time = time.time()\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_rf = gs.best_estimator_\n",
    "    best_score = gs.best_estimator_.oob_score_\n",
    "    feature_importance = pd.DataFrame({'importance':\\\n",
    "                                       best_rf.feature_importances_,\\\n",
    "                                       'feature': variables_lista})\n",
    "    feature_importance=feature_importance.sort_values(by=\"importance\", ascending=False)\n",
    "    \n",
    "    #fi_out = feature_importance.head(10)\n",
    "    \n",
    "    time_exec = time.time() - start_time\n",
    "    nrows_ohe = data_input_ohe.shape[0]\n",
    "    ncols_ohe = data_input_ohe.shape[1]\n",
    "    \n",
    "    #print(\"Tiempo en ejecutar: \", time.time() - start_time)\n",
    "    \n",
    "    return df_input, nrows_ohe, ncols_ohe, float(best_score), time_exec, str(best_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-richardson",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "radio-documentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df_fe):\n",
    "    # Aplicamos OneHot Encoding\n",
    "    data_input_ohe = pd.get_dummies(df_fe)\n",
    "    etiqueta = data_input_ohe.label_results\n",
    "    data_input_ohe= data_input_ohe.drop('label_results', axis = 1)\n",
    "    variables_lista = list(data_input_ohe.columns)\n",
    "    # Hacemos TimeSeriesSplit para obtener las matrices de entrenamiento y prueba\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    for train_index, test_index in tscv.split(data_input_ohe):\n",
    "        X_train, X_test = data_input_ohe.iloc[train_index, :], data_input_ohe.iloc[test_index,:]\n",
    "        y_train, y_test = etiqueta.iloc[train_index], etiqueta.iloc[test_index]\n",
    "    # Metadata de las matrices para el modelo\n",
    "    nrows_train = X_train.shape[0]\n",
    "    nrows_test = X_test.shape[0]\n",
    "    meta_train = pd.DataFrame({'nrows_train' : nrows_train,\n",
    "                           'nrows_test' : nrows_test}, index = [0])\n",
    "    # Hacemos un solo DF para los Datasets de Entrenamiento y Prueba y con la etiqueta\n",
    "    X_train_1 = X_train.assign(Set = 'entrenamiento')\n",
    "    X_train_1 = X_train_1.assign(etiqueta = y_train)\n",
    "    X_test_1 = X_test.assign(Set = 'prueba')\n",
    "    X_test_1 = X_test_1.assign(etiqueta = y_test)\n",
    "    df_train_test = pd.concat([X_train_1, X_test_1], axis = 0)\n",
    "    return df_train_test, nrows_train, nrows_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-emerald",
   "metadata": {},
   "source": [
    "## Selección de Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "precious-picnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(df_train_test):\n",
    "    # Funciones para regresar el DataFrame con las etiquetas y los sets de entrenamiento y \n",
    "    # y prueba a los cuatro DF X_train, Y_train, X_test y Y_test\n",
    "    X_train = df_train_test[df_train_test.Set == 'entrenamiento']\n",
    "    y_train = X_train.etiqueta\n",
    "    X_train = X_train.iloc[:,0:df_train_test.shape[1]-2]\n",
    "    X_test = df_train_test[df_train_test.Set == 'prueba']\n",
    "    #y_test = X_test.etiqueta\n",
    "    X_test = X_test.iloc[:,0:df_train_test.shape[1]-2]\n",
    "    # Algoritmos a evaluar: DecisionTree y RandomForest\n",
    "    algorithms_dict = {'tree': 'tree_grid_search'}\n",
    "    algorithms = ['tree']\n",
    "    # Hiperparámetros a evaluar en cada algoritmo:\n",
    "    grid_search_dict = {'tree_grid_search': {'max_depth': [5,10,15], \n",
    "                                         'min_samples_leaf': [3,5,7]}}\n",
    "    # Configuraciones generales de cada algoritmo a evaluar:\n",
    "    estimators_dict = {'tree': DecisionTreeClassifier(random_state=1111)}\n",
    "    best_estimators = []\n",
    "    # Magic loop\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    start_time = time.time()\n",
    "    for algorithm in algorithms:\n",
    "        estimator = estimators_dict[algorithm]\n",
    "        grid_search_to_look = algorithms_dict[algorithm]\n",
    "        grid_params = grid_search_dict[grid_search_to_look]\n",
    "        gs = GridSearchCV(estimator, grid_params, scoring='precision', cv=tscv, n_jobs=-1)\n",
    "        #train\n",
    "        gs.fit(X_train, y_train)\n",
    "        #best estimator\n",
    "        best_estimators.append(gs)\n",
    "    \n",
    "    # MEtadata:\n",
    "    # Mejor modelo de árbol\n",
    "    best_tree = best_estimators[0].best_estimator_\n",
    "    # Tiempo de ejecución\n",
    "    t_exec = time.time() - start_time\n",
    "    # Información de los modelos considerados en la selección\n",
    "    r = pd.DataFrame(best_estimators[0].cv_results_)\n",
    "    r = r.sort_values(\"rank_test_score\")\n",
    "    lista = r.params.astype(str)\n",
    "    test_mod = \"|\".join(lista)\n",
    "    lista_2 = r.mean_test_score.astype(str)\n",
    "    mean_scores = \"|\".join(lista_2)\n",
    "    lista_3 = r.rank_test_score.astype(str)\n",
    "    rank_model = \"|\".join(lista_3)\n",
    "    # Persistir mejor modelo en .pkl\n",
    "    obj_model = best_tree.fit(X_train, y_train)\n",
    "#    pickle.dump(best_tree, open(\"best_model.pkl\", 'wb'))\n",
    "#    model_pkl = pickle.load(open(\"best_model.pkl\",\"rb\"))\n",
    "    return obj_model, str(best_tree), t_exec, test_mod, mean_scores, rank_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-major",
   "metadata": {},
   "source": [
    "# Sesgo e Inequidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "regulated-taste",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_fair(modelo,df_train_test,df_fe):\n",
    "    # Separamos los sets de entrenamiento y prueba\n",
    "    # Entrenamiento\n",
    "    X_train = df_train_test[df_train_test.Set == 'entrenamiento']\n",
    "    y_train = X_train.etiqueta\n",
    "    X_train = X_train.iloc[:,0:df_train_test.shape[1]-2]\n",
    "    # Prueba\n",
    "    X_test = df_train_test[df_train_test.Set == 'prueba']\n",
    "    y_test = X_test.etiqueta\n",
    "    X_test = X_test.iloc[:,0:df_train_test.shape[1]-2]\n",
    "    predicted_scores = modelo.predict_proba(X_test)\n",
    "    \n",
    "    # Se conforma el DataFrame que necesita Aequitas para el sesgo e inequidad de la variable facility_type (class)\n",
    "    df_dummy = pd.DataFrame()\n",
    "    df_dummy['scores'] = pd.Series(predicted_scores[:,1])\n",
    "    df_dummy['predic'] = np.where(df_dummy['scores'] < 0.7,0,1)\n",
    "    df_aeq = pd.DataFrame()\n",
    "    df_aeq['real'] = y_test\n",
    "    df_aeq['prediccion'] = df_dummy.predic\n",
    "    df_aeq['faciliy_type'] = df_fe['class'].tail(len(df_dummy.predic))\n",
    "    # Asignamos nuevos índices y los nombres de las columnas para que los reconozca la función\n",
    "    df_aeq = df_aeq.reset_index(drop=True)\n",
    "    df_aeq.columns = ['label_value','score','class']\n",
    "    # Se obtienen las métricas\n",
    "    g = Group()\n",
    "    xtab, attrbs = g.get_crosstabs(df_aeq)\n",
    "    absolute_metrics = g.list_absolute_metrics(xtab)\n",
    "    metrics1 = xtab[['attribute_name', 'attribute_value']+[col for col in xtab.columns if col in absolute_metrics]].round(2)\n",
    "    \n",
    "    # Se conforma el DataFrame que necesita Aequitas para el sesgo e inequidad de la variable zip (level)\n",
    "    df_aeq2 = pd.DataFrame()\n",
    "    df_aeq2['real'] = y_test\n",
    "    df_aeq2['prediccion'] = df_dummy.predic\n",
    "    df_aeq2['zip'] = df_fe['level'].tail(len(df_dummy.predic))\n",
    "    # Asignamos nuevos índices y los nombres de las columnas para que los reconozca la función\n",
    "    df_aeq2 = df_aeq2.reset_index(drop=True)\n",
    "    df_aeq2.columns = ['label_value','score','level']\n",
    "    # Se obtienen las métricas\n",
    "    g2 = Group()\n",
    "    xtab2, attrbs2 = g2.get_crosstabs(df_aeq2)\n",
    "    absolute_metrics2 = g2.list_absolute_metrics(xtab2)\n",
    "    metrics2 = xtab2[['attribute_name', 'attribute_value']+[col for col in xtab2.columns if col in absolute_metrics2]].round(2)\n",
    "    \n",
    "    df_labels = pd.DataFrame()\n",
    "    df_labels['scores'] = pd.Series(predicted_scores[:,1])\n",
    "    df_labels['predicted'] = np.where(df_dummy['scores'] < 0.7,0,1)\n",
    "    df_labels['label'] = y_test\n",
    "    \n",
    "    metrics = pd.concat([metrics1,metrics2]).reset_index(drop = True)\n",
    "        \n",
    "    # Metadata\n",
    "    n_groups = len(metrics1.attribute_value) + len(metrics2.attribute_value)\n",
    "    n_attribute = metrics.attribute_name.nunique()\n",
    "    prop_pos_pred = df_labels.predicted.sum()/len(df_labels.predicted)\n",
    "    prop_pos_real = df_labels.label.sum()/len(df_labels.label)\n",
    "        \n",
    "    return df_labels, metrics, n_groups, n_attribute, prop_pos_pred, prop_pos_real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-panel",
   "metadata": {},
   "source": [
    "### Data Frame original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "precious-academy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 215472 entries, 0 to 215471\n",
      "Data columns (total 17 columns):\n",
      " #   Column           Non-Null Count   Dtype \n",
      "---  ------           --------------   ----- \n",
      " 0   inspection_id    215472 non-null  object\n",
      " 1   dba_name         215472 non-null  object\n",
      " 2   aka_name         213010 non-null  object\n",
      " 3   license_         215455 non-null  object\n",
      " 4   facility_type    210599 non-null  object\n",
      " 5   risk             215403 non-null  object\n",
      " 6   address          215472 non-null  object\n",
      " 7   city             215317 non-null  object\n",
      " 8   state            215422 non-null  object\n",
      " 9   zip              215428 non-null  object\n",
      " 10  inspection_date  215472 non-null  object\n",
      " 11  inspection_type  215471 non-null  object\n",
      " 12  results          215472 non-null  object\n",
      " 13  latitude         214739 non-null  object\n",
      " 14  longitude        214739 non-null  object\n",
      " 15  location         214739 non-null  object\n",
      " 16  violations       157923 non-null  object\n",
      "dtypes: object(17)\n",
      "memory usage: 27.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_raw = pickle.load(open(\"raw.pkl\",\"rb\"))\n",
    "df_raw = pd.DataFrame(df_raw)\n",
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "quantitative-slide",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\envs\\dpa\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\Usuario\\anaconda3\\envs\\dpa\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "df2, nrows_prev, ncols_prev, nrows_after, ncols_after, data_null_prev = cleaning(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "limited-universe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 154326 entries, 0 to 154326\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Non-Null Count   Dtype         \n",
      "---  ------            --------------   -----         \n",
      " 0   aka_name          154326 non-null  object        \n",
      " 1   license           154326 non-null  object        \n",
      " 2   facility_type     154326 non-null  object        \n",
      " 3   label_risk        154326 non-null  int32         \n",
      " 4   zip               154326 non-null  object        \n",
      " 5   inspection_date   154326 non-null  datetime64[ns]\n",
      " 6   inspection_type   154326 non-null  object        \n",
      " 7   violations_count  154326 non-null  int32         \n",
      " 8   sin_mnth          154326 non-null  float64       \n",
      " 9   cos_mnth          154326 non-null  float64       \n",
      " 10  sin_wkd           154326 non-null  float64       \n",
      " 11  cos_wkd           154326 non-null  float64       \n",
      " 12  label_results     154326 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(4), int32(2), int64(1), object(5)\n",
      "memory usage: 15.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "offshore-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input, nrows_ohe, ncols_ohe, best_score, time_exec, best_rf = feat_eng(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "tired-radio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "286.47401309013367"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "assigned-teens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 154326 entries, 2788 to 0\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   label_risk     154326 non-null  int32 \n",
      " 1   label_results  154326 non-null  int64 \n",
      " 2   level          154326 non-null  object\n",
      " 3   class          154326 non-null  object\n",
      "dtypes: int32(1), int64(1), object(2)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_input.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "afraid-provider",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    112565\n",
       "2     29756\n",
       "1     12000\n",
       "0         5\n",
       "Name: label_risk, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input.label_risk.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "written-lying",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    112761\n",
       "0     41565\n",
       "Name: label_results, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_input.label_results.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "complicated-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_test, nrows_train, nrows_test = train(df_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "million-assist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 154326 entries, 2788 to 0\n",
      "Data columns (total 13 columns):\n",
      " #   Column                              Non-Null Count   Dtype \n",
      "---  ------                              --------------   ----- \n",
      " 0   label_risk                          154326 non-null  int32 \n",
      " 1   level_downtown                      154326 non-null  uint8 \n",
      " 2   level_high                          154326 non-null  uint8 \n",
      " 3   level_low-mid                       154326 non-null  uint8 \n",
      " 4   level_other                         154326 non-null  uint8 \n",
      " 5   class_children's services facility  154326 non-null  uint8 \n",
      " 6   class_daycare                       154326 non-null  uint8 \n",
      " 7   class_grocery store                 154326 non-null  uint8 \n",
      " 8   class_other                         154326 non-null  uint8 \n",
      " 9   class_restaurant                    154326 non-null  uint8 \n",
      " 10  class_school                        154326 non-null  uint8 \n",
      " 11  Set                                 154326 non-null  object\n",
      " 12  etiqueta                            154326 non-null  int64 \n",
      "dtypes: int32(1), int64(1), object(1), uint8(10)\n",
      "memory usage: 5.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "broke-charge",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_model, best_tree, t_exec, test_mod, mean_scores, rank_model = model(df_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dominican-marketing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=10, min_samples_leaf=3, random_state=1111)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "certified-latter",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels, metrics, n_groups, n_attribute, prop_pos_pred, prop_pos_real = bias_fair(obj_model,df_train_test,df_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "sharp-pride",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6809999999999998"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "equivalent-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(df2,open(\"limpieza.pkl\",\"wb\"))             # DF limpio\n",
    "pickle.dump(df_input,open(\"feat_eng.pkl\",\"wb\"))        # DF con el Feature Engineering\n",
    "pickle.dump(df_train_test,open(\"train_test.pkl\",\"wb\")) # Entrenamiento y Prueba\n",
    "pickle.dump(obj_model,open(\"obj_model.pkl\",\"wb\"))      # Selección del Modelo\n",
    "pickle.dump(metrics,open(\"metrics.pkl\",\"wb\"))          # Métricas de desempeño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-organizer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-guidance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-forward",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-reform",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pickle.load(open(\"df_raw.pkl\",\"rb\"))\n",
    "df_clean = pickle.load(open(\"df_clean.pkl\",\"rb\"))\n",
    "df_fe = pickle.load(open(\"df_fe.pkl\",\"rb\"))\n",
    "m = pickle.load(open(\"best_model.pkl\",\"rb\"))\n",
    "df_trn_tst = pickle.load(open(\"df_trn_tst.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-jewelry",
   "metadata": {},
   "source": [
    "**Transformación a OHE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-raise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle con la base de datos original\n",
    "#pickle.dump(df,open(\"df_raw.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpia la Base de Datos\n",
    "#df_raw = pickle.load(open(\"df_raw.pkl\",\"rb\"))\n",
    "#df_clean, nrows_prev, ncols_prev, nrows_after, ncols_after, data_null_prev = cleaning(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-madonna",
   "metadata": {},
   "source": [
    "**Extracción de datos del último día de 2020 hacia atrás**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-criterion",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pickle.load(open(\"df_clean.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = df_clean.loc[df_clean.inspection_date > pd.to_datetime('2020-12-31'), :].index\n",
    "df_clean = df_clean.drop(var,axis=0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta_clean = pd.DataFrame({'nrows_prev' : nrows_prev,\n",
    "#                        'ncols_prev' : ncols_prev,\n",
    "#                        'nrows_after' : nrows_after,\n",
    "#                        'ncols_after' : ncols_after,\n",
    "#                        'data_null_prev' : data_null_prev}, index = [0])\n",
    "#meta_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-bloom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickles con la base de datos limpia y su metadata\n",
    "pickle.dump(df_clean,open(\"df_clean.pkl\",\"wb\"))\n",
    "#pickle.dump(meta_clean,open(\"meta_clean.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-macro",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_fe, nrows_ohe, ncols_ohe, best_score, time_exec, best_rf = feat_eng(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-detective",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-airline",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-mileage",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_fe = pd.DataFrame({'nrows_ohe' : nrows_ohe,\n",
    "                        'ncols_ohe' : ncols_ohe,\n",
    "                        'best_score' : best_score,\n",
    "                        'time_exec' : time_exec,\n",
    "                        'best_rf' : best_rf}, index = [0])\n",
    "meta_fe                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-bulgaria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickles con Feature Engineering y su metadata\n",
    "pickle.dump(df_fe,open(\"df_fe.pkl\",\"wb\"))\n",
    "pickle.dump(meta_fe,open(\"meta_fe.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "df_fe = pickle.load(open(\"df_fe.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos OneHot Encoding\n",
    "data_input_ohe = pd.get_dummies(df_fe)\n",
    "etiqueta = data_input_ohe.label_results\n",
    "data_input_ohe= data_input_ohe.drop('label_results', axis = 1)\n",
    "variables_lista = list(data_input_ohe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos TimeSeriesSplit para obtener las matrices de entrenamiento y prueba\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "for train_index, test_index in tscv.split(data_input_ohe):\n",
    "    X_train, X_test = data_input_ohe.iloc[train_index, :], data_input_ohe.iloc[test_index,:]\n",
    "    y_train, y_test = etiqueta.iloc[train_index], etiqueta.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-yield",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata de las matrices para el modelo\n",
    "nrows_train = X_train.shape[0]\n",
    "nrows_test = X_test.shape[0]\n",
    "meta_train = pd.DataFrame({'nrows_train' : nrows_train,\n",
    "                           'nrows_test' : nrows_test}, index = [0])\n",
    "meta_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos un solo DF para los Datasets de Entrenamiento y Prueba y con la etiqueta\n",
    "X_train_1 = X_train.assign(Set = 'entrenamiento')\n",
    "X_train_1 = X_train_1.assign(etiqueta = y_train)\n",
    "X_test_1 = X_test.assign(Set = 'prueba')\n",
    "X_test_1 = X_test_1.assign(etiqueta = y_test)\n",
    "df_train_test = pd.concat([X_train_1, X_test_1], axis = 0)\n",
    "\n",
    "df_train_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-cartoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(df_train_test.etiqueta) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(df_train_test,open(\"df_trn_tst.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones para regresar el DataFrame con las etiquetas y los sets de entrenamiento y \n",
    "# y prueba a los cuatro DF Xtrain, Ytrain, Xtst y Ytest\n",
    "X_train_2 = df_train_test[df_train_test.Set == 'entrenamiento']\n",
    "y_train_2 = X_train_2.etiqueta\n",
    "X_train_2 = X_train_2.iloc[:,0:df_train_test.shape[1]-2]\n",
    "\n",
    "\n",
    "X_test_2 = df_train_test[df_train_test.Set == 'prueba']\n",
    "y_test_2 = X_test_2.etiqueta\n",
    "X_test_2 = X_test_2.iloc[:,0:df_train_test.shape[1]-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-narrative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmos a evaluar: DecisionTree y RandomForest\n",
    "algorithms_dict = {'tree': 'tree_grid_search'}\n",
    "algorithms = ['tree']\n",
    "# Hiperparámetros a evaluar en cada algoritmo:\n",
    "grid_search_dict = {'tree_grid_search': {'max_depth': [5,10,15], \n",
    "                                         'min_samples_leaf': [3,5,7]}}\n",
    "\n",
    "# Configuraciones generales de cada algoritmo a evaluar:\n",
    "estimators_dict = {'tree': DecisionTreeClassifier(random_state=1111)}\n",
    "best_estimators = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-differential",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for algorithm in algorithms:\n",
    "    estimator = estimators_dict[algorithm]\n",
    "    grid_search_to_look = algorithms_dict[algorithm]\n",
    "    grid_params = grid_search_dict[grid_search_to_look]\n",
    "    gs = GridSearchCV(estimator, grid_params, scoring='precision', cv=tscv, n_jobs=-1)\n",
    "    start_time = time.time()\n",
    "    #train\n",
    "    gs.fit(X_train, y_train)\n",
    "    #best estimator\n",
    "    best_estimators.append(gs)\n",
    "time_exec = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-female",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tree = best_estimators[0].best_estimator_\n",
    "best_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-qatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.DataFrame(best_estimators[0].cv_results_)\n",
    "r = r.sort_values(\"rank_test_score\")\n",
    "r.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-justice",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = r.params.astype(str)\n",
    "test_mod = \"|\".join(lista)\n",
    "lista_2 = r.mean_test_score.astype(str)\n",
    "mean_scores = \"|\".join(lista_2)\n",
    "lista_3 = r.rank_test_score.astype(str)\n",
    "rank_model = \"|\".join(lista_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-health",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-blade",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-affiliate",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla = pd.DataFrame({\"best\":best_tree,\"modelos\":test_mod,\"mean_scores\":mean_scores,\n",
    "                      \"rank_model\":rank_model},index=[0])\n",
    "tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model.fit(X_train, y_train)\n",
    "pickle.dump(m, open(\"best_model.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pickle.load(open(\"best_model.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-webmaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = m.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(resultados)\n",
    "res=res.rename(columns = {0:'label'})\n",
    "res.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-trout",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_2 = pd.DataFrame(y_test)\n",
    "res_2.label_results.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-bobby",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-syndication",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels, metrics, n_groups, n_attribute, prop_pos_pred, prop_pos_real = bias_fair(m,df_trn_tst,df_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-significance",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-recognition",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_pos_pred = df_labels.predicted.sum()/len(df_labels.predicted)\n",
    "prop_pos_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_pos_real = df_labels.label.sum()/len(df_labels.label)\n",
    "prop_pos_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-reconstruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento\n",
    "X_train = df_trn_tst[df_trn_tst.Set == 'entrenamiento']\n",
    "y_train = X_train.etiqueta\n",
    "X_train = X_train.iloc[:,0:df_trn_tst.shape[1]-2]\n",
    "# Prueba\n",
    "X_test = df_trn_tst[df_trn_tst.Set == 'prueba']\n",
    "y_test = X_test.etiqueta\n",
    "X_test = X_test.iloc[:,0:df_trn_tst.shape[1]-2]\n",
    "predicted_scores = m.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummy = pd.DataFrame()\n",
    "df_dummy['scores'] = pd.Series(predicted_scores[:,1])\n",
    "df_dummy['predic'] = np.where(df_dummy['scores'] < 0.7,0,1)\n",
    "df_dummy.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aeq2 = pd.DataFrame()\n",
    "df_aeq2['real'] = y_test\n",
    "df_aeq2['prediccion'] = df_dummy.predic\n",
    "df_aeq2['zip'] = df_fe['level'].tail(len(df_dummy.predic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aeq2 = df_aeq2.reset_index(drop=True)\n",
    "df_aeq2.columns = ['label_value','score','level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-lightweight",
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = Group()\n",
    "xtab2, attrbs2 = g2.get_crosstabs(df_aeq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-storage",
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_metrics2 = g2.list_absolute_metrics(xtab2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtab2[[col for col in xtab2.columns if col not in absolute_metrics2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-support",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics2 = xtab2[['attribute_name', 'attribute_value']+[col for col in xtab2.columns if col in absolute_metrics2]].round(2)\n",
    "metrics2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-delight",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = pd.concat([metrics2,metrics2]).reset_index(drop = True)\n",
    "con.attribute_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "aeq2 = Plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_2 = aeq2.plot_group_metric(xtab2, 'for')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall2 = aeq2.plot_group_metric(xtab2, 'tpr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-lancaster",
   "metadata": {},
   "source": [
    "**¿Cuáles son los atributos protegidos?**\n",
    "\n",
    "Se hicieron dos ejercicios. El primero consistió en tomar como atributo protegido el tipo de establecimiento (facility_type). Para generar esta variable, dado que existen 500 tipos de establecimiento, se clasificaron los grupos de mayor representación en la base de datos, resultando 4 los de mayor aparición (\"restaurant\",\"school\", \"grocery store\", \"children's services facility\"), después se agruparon tres categorías que se refieren a \"day care\" y el resto se clasificó como \"other\", esto por la heterogeneidad de establecimientos que ya no entraban en las categorías previas.\n",
    "\n",
    "El segundo consistió en tomar como atributo protegido el código postal (zip), para ello se creó una tabla de códigos postales clasificados por tipo de ingreso: \"High\", \"Low-mid\" y \"Down town\" (ver referencias) y, debido a que hay algunos códigos postales que se encuentran fuera del área de Chicago y no contamos con su clasificación de ingreso, se dejó en una categoría denominada \"other\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-boundary",
   "metadata": {},
   "source": [
    "**¿Qué grupos de referencia tiene cada atributo protegido?, explica el por qué**\n",
    "\n",
    "Para el atributo \"facility_type\", el grupo de referencia es la categoría de \"restaurante\", porque es la que tiene mayor representación en la base de datos y el objetivo sería que no haya sesgo en las predicciones con etiqueta negativa hacia este tipo de establecimiento.\n",
    "\n",
    "Para el atributo \"zip\", el grupo de referencia es la categoría de \"low-mid\" pues el objetivo sería que no haya sesgo en las predicciones con etiqueta negativa hacia este tipo de zonas (con menor ingreso), que pudieran generar mayor disparidad respecto a las demás zonas (\"High\" y \"Downtown\") tomando en cuenta que la cancelación de licencias de restaurantes puede afectar sensiblemente la economía o el desarrollo de alguna zona."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-shield",
   "metadata": {},
   "source": [
    "**¿Tu modelo es punitivo o asistivo? explica por qué**\n",
    "\n",
    "De acuerdo con la pregunta análitica que se desea responder con el modelo predictivo (¿El establecimiento pasará o no la inspección?) y tomando en cuenta que el producto de datos estará orientado para que el uso sea por parte de los establecimientos y no del gobierno de Chicago para enviar inspecciones, consideramos que el modelo es asistivo ya que permitirá a los dueños de los establecimientos realizar consultas sobre si su establecimiento pasaría o no una inspección, permitiéndoles prevenir posibles multas o cancelaciones de licencia por incumplimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-delhi",
   "metadata": {},
   "source": [
    "**¿Qué métricas cuantificarás y ocuparás en sesgo e inequidad? explica por qué**\n",
    "\n",
    "- ***Equal Parity***: Esta métrica no es tan útil para nuestro objetivo porque nos interesa mejorar ninguno de los grupos, solo nos interesa orientar a los usuarios sobre si su establecimiento está o no en riesgo de pasar una posible inspección.\n",
    "\n",
    "- ***False Positive Parity***: \n",
    "\n",
    "\n",
    "- ***FNR Parity***: la interpretamos de la siguiente manera; dado que se recibe una alerta falsa, cuál es la probabilidad de que hayamos enviado una ambulancia dado la delegación a la que pertenece la llamada. Seleccionamos la métrica porque queremos verificar si alguna delegación tiene ventaja sobre otra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-machinery",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-toilet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-painting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmos a evaluar: DecisionTree y RandomForest\n",
    "algorithms_dict = {'random_forest': 'rf_grid_search'}\n",
    "algorithms = ['random_forest']\n",
    "# Hiperparámetros a evaluar en cada algoritmo:\n",
    "grid_search_dict = {'rf_grid_search': {'n_estimators': [500],  \n",
    "                                      'max_depth': [5,10], \n",
    "                                      'min_samples_leaf': [10]}}\n",
    "\n",
    "# Configuraciones generales de cada algoritmo a evaluar:\n",
    "estimators_dict = {'random_forest': RandomForestClassifier(oob_score=True, random_state=2222)}\n",
    "best_estimators = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-central",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for algorithm in algorithms:\n",
    "    estimator = estimators_dict[algorithm]\n",
    "    grid_search_to_look = algorithms_dict[algorithm]\n",
    "    grid_params = grid_search_dict[grid_search_to_look]\n",
    "    gs = GridSearchCV(estimator, grid_params, scoring='precision', cv=tscv, n_jobs=-1)\n",
    "    start_time = time.time()\n",
    "    #train\n",
    "    gs.fit(X_train, y_train)\n",
    "    #best estimator\n",
    "    best_estimators.append(gs)\n",
    "time_exec = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = best_estimators[0].best_estimator_\n",
    "best_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = best_rf\n",
    "m_rf = model_rf.fit(X_train, y_train)\n",
    "pickle.dump(m, open(\"best_model_rf.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-complaint",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_rf = best_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_rf = pd.DataFrame(resultados_rf)\n",
    "res_rf = res_rf.rename(columns = {0:'label'})\n",
    "res_rf.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = pickle.load(open(\"consecutiva.pkl\",\"rb\"))\n",
    "type(cons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-external",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
