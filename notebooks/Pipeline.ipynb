{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "copyrighted-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib.pyplot import figure\n",
    "from sodapy import Socrata\n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, KBinsDiscretizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "import pickle\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Socrata(\"data.cityofchicago.org\", None)\n",
    "results = client.get(\"4ijn-s7e5\", limit=400000)\n",
    "df = pd.DataFrame.from_records(results)\n",
    "col_names = df.columns.to_list()\n",
    "col_name = []\n",
    "for i in range(len(col_names)):\n",
    "    col_name.append(col_names[i].replace(\" \", \"_\").lower())\n",
    "df.columns =col_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-fishing",
   "metadata": {},
   "source": [
    "## Limpieza y preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-luxury",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarize_column_strings(df, columns, excluded_punctuation=\".,*¿?¡!\"):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(\" \", \"_\")\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(\"-\", \"_\")\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(\"á\", \"a\")\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(\"é\", \"e\")\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(\"í\", \"i\")\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(\"ó\", \"o\")\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(\"ú\", \"u\")\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(\"ü\", \"u\")\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(r\"[^a-zA-Z\\d\\_]+\", \"\")\n",
    "        for ch in excluded_punctuation:\n",
    "            df[col] = df[col].str.replace(ch, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(df):\n",
    "    '''\n",
    "    Función que convierte las columnas del Data Frame al tipo y forma que se necesita para\n",
    "    los análisis posteriores\n",
    "    \n",
    "    inputs: Data Frame almacenado en el S3 (ingesta.pkl)\n",
    "    outputs: Data Frame con las variables en formato adecuado (df_clean.pkl)\n",
    "        \n",
    "    '''\n",
    "    #df = pickle.load(open(\"ingesta.pkl\",\"rb\"))\n",
    "    nrows_prev = df.shape[0]\n",
    "    ncols_prev = df.shape[1]\n",
    "    data_null_prev = df.isnull().sum().sum()\n",
    "    # Variables de texto\n",
    "    df['violations']= df['violations'].astype('object')\n",
    "    df['violations_count'] = df.violations.str.count(r'\\|')+1\n",
    "    df['violations_count'] = df.violations_count.fillna(0)\n",
    "    df['violations_count'] = df['violations_count'].astype('int')\n",
    "    # Variables categóricas\n",
    "    df['dba_name']= df['dba_name'].astype('object')\n",
    "    df['aka_name']= df['aka_name'].astype('object')\n",
    "    df['facility_type']= df['facility_type'].astype('category')\n",
    "    df['risk']= df['risk'].astype('category')\n",
    "    df['address']= df['address'].astype('category')\n",
    "    df['city']= df['city'].astype('category')\n",
    "    df['state']= df['state'].astype('category')\n",
    "    df['inspection_type']= df['inspection_type'].astype('category')\n",
    "    df['results']= df['results'].astype('category')\n",
    "    # Variable label_risk\n",
    "    df['risk'] = df['risk'].replace([\"Risk 1 (High)\"],3)\n",
    "    df['risk'] = df['risk'].replace([\"Risk 2 (Medium)\"],2)\n",
    "    df['risk'] = df['risk'].replace([\"Risk 3 (Low)\"],1)\n",
    "    df['risk'] = df['risk'].replace([\"All\"],0)\n",
    "    df['risk'] = pd.to_numeric(df['risk'], errors='coerce')\n",
    "    df=df.rename(columns = {'risk':'label_risk'})\n",
    "    df['label_risk'] = df['label_risk'].fillna(3)\n",
    "    df['label_risk'] = df['label_risk'].astype('int')\n",
    "    # Variables de fecha\n",
    "    df['inspection_date'] = pd.to_datetime(df['inspection_date'], infer_datetime_format=True)\n",
    "    df['inspection_month']=df['inspection_date'].dt.month\n",
    "    MONTH = 12\n",
    "    df['sin_mnth'] = np.sin(2*np.pi*df.inspection_month/MONTH)\n",
    "    df['cos_mnth'] = np.cos(2*np.pi*df.inspection_month/MONTH)\n",
    "    df['inspection_weekday']=df['inspection_date'].dt.weekday\n",
    "    WEEKDAY = 7\n",
    "    df['sin_wkd'] = np.sin(2*np.pi*df.inspection_weekday/WEEKDAY)\n",
    "    df['cos_wkd'] = np.cos(2*np.pi*df.inspection_weekday/WEEKDAY)\n",
    "    # Etiqueta\n",
    "    df['label_results'] = df['results'].apply(lambda x: int(0) if x == 'Fail' else (int(1) if x in ['Pass','Pass w/Conditions'] else int(2)))\n",
    "    # Imputación de datos\n",
    "    df.drop(['violations'],axis = 1, inplace = True)\n",
    "    df.drop(['results'], axis = 1, inplace = True)\n",
    "    df.drop(df.loc[df['license_'].isnull()].index, inplace=True)\n",
    "    df.drop(df.loc[df['zip'].isnull()].index, inplace=True)\n",
    "    df.drop(df.loc[df['label_results'] == 2].index, inplace=True)\n",
    "    df['aka_name'] = df['aka_name'].fillna(df['dba_name'])\n",
    "    df['dba_name']= df['dba_name'].astype(str).str.lower()\n",
    "    df['aka_name']= df['aka_name'].astype(str).str.lower()\n",
    "    df['facility_type']= df['facility_type'].astype(str).str.lower()\n",
    "    df['state']= df['state'].astype(str).str.lower()\n",
    "    df['inspection_type']= df['inspection_type'].astype(str).str.lower()\n",
    "    df = df[~df['state'].isin(['wi', 'ny', 'in'])]\n",
    "    col_text = ['dba_name','aka_name']\n",
    "    # Eliminamos el '_' que aparece al final en la columna 'license_'\n",
    "    df.rename(columns={'license_':'license'}, inplace=True)\n",
    "    standarize_column_strings(df, col_text)\n",
    "    df_dict_dummy = pd.DataFrame(df['aka_name'])\n",
    "    df_dict_dummy['facility_type'] = df['facility_type']\n",
    "    df_dict_dummy.drop(df_dict_dummy.loc[df_dict_dummy['facility_type'].isnull()].index, inplace=True)\n",
    "    group = df_dict_dummy.groupby('aka_name')\n",
    "    df_dict_dummy2 = group.apply(lambda x: x['facility_type'].unique())\n",
    "    df_dict_dummy3 = df_dict_dummy2.to_frame()\n",
    "    df_dict_dummy3.reset_index(level = 'aka_name', inplace = True)\n",
    "    df_dict_dummy3 = df_dict_dummy3.rename(columns = {0:'facility_type'})\n",
    "    df_dict_dummy3['facility_type'] = df_dict_dummy3['facility_type'].apply(lambda x: str(x[0]))\n",
    "    df2 = pd.merge(df,df_dict_dummy3, how = 'left', on = 'aka_name')\n",
    "    df2['facility_type_x'] = df2['facility_type_x'].fillna(df2['facility_type_y'])\n",
    "    df2['facility_type_x'] = df2['facility_type_x'].fillna('restaurant')\n",
    "    df2=df2.rename(columns = {'facility_type_x':'facility_type'})\n",
    "    df2.drop(['inspection_id','dba_name','address','city','state','latitude','longitude','location','facility_type_y','inspection_weekday','inspection_month'],axis = 1, inplace = True)\n",
    "    #pickle.dump(df2,open(\"df_clean.pkl\",\"wb\"))\n",
    "    nrows_after = df2.shape[0]\n",
    "    ncols_after = df2.shape[1]\n",
    "    return df2, nrows_prev, ncols_prev, nrows_after, ncols_after, data_null_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-crystal",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-advance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_eng(df_fe):\n",
    "    '''\n",
    "    Función que realiza la selección de los features que serán utilizdos para la clasificación\n",
    "    \n",
    "    inputs: Data Frame limpio (df_clean.pkl)\n",
    "    outputs: Data Frame con la matriz de diseño para el modelo (df_clean.pkl)\n",
    "        \n",
    "    '''\n",
    "     \n",
    "    # Transformación a OHE\n",
    "    df_fe = df_fe.sort_values(by='inspection_date', ascending=True)\n",
    "    df_input = pd.DataFrame(df_fe[['label_risk','label_results','zip','facility_type']])\n",
    "    data_input_ohe = pd.get_dummies(df_input)\n",
    "    etiqueta = data_input_ohe.label_results\n",
    "    data_input_ohe= data_input_ohe.drop('label_results', axis = 1)\n",
    "    variables_lista = list(data_input_ohe.columns)\n",
    "    # Grid Search\n",
    "    np.random.seed(20201124)\n",
    "    # ocuparemos un RF\n",
    "    classifier = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=1234)\n",
    "    # separando en train, test\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(data_input_ohe, etiqueta, test_size=0.3)\n",
    "\n",
    "    # definicion de los hiperparametros que queremos probar\n",
    "    hyper_param_grid = {'n_estimators': [300, 400], #'min_samples_leaf': [3,5,7],\n",
    "                        'max_depth': [7, 10],\n",
    "                        'min_samples_split': [3],\n",
    "                        'max_features': [10, 15, 20],\n",
    "                        'criterion': ['gini']}\n",
    "    # usamos TimeSeriesSplit para dividir respetando el orden cronológico\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    # This was the trickiest part as a newbie. Straight from the docs\n",
    "    # If you only have experience with CV splits this way\n",
    "    # of making the splits might seem foreign. Fret not.\n",
    "    for train_index, test_index in tscv.split(data_input_ohe):\n",
    "        X_train, X_test = data_input_ohe.iloc[train_index, :], data_input_ohe.iloc[test_index,:]\n",
    "        y_train, y_test = etiqueta.iloc[train_index], etiqueta.iloc[test_index]\n",
    "    # ocupemos grid search\n",
    "    gs = GridSearchCV(classifier, \n",
    "                           hyper_param_grid, \n",
    "                           scoring = 'precision', return_train_score=True,\n",
    "                           cv = tscv)\n",
    "    start_time = time.time()\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_rf = gs.best_estimator_\n",
    "    best_score = gs.best_estimator_.oob_score_\n",
    "    feature_importance = pd.DataFrame({'importance':\\\n",
    "                                       best_rf.feature_importances_,\\\n",
    "                                       'feature': variables_lista})\n",
    "    feature_importance=feature_importance.sort_values(by=\"importance\", ascending=False)\n",
    "    #fi_out = feature_importance.head(10)\n",
    "    time_exec = time.time() - start_time\n",
    "    nrows_ohe = data_input_ohe.shape[0]\n",
    "    ncols_ohe = data_input_ohe.shape[1]\n",
    "    #print(\"Tiempo en ejecutar: \", time.time() - start_time)\n",
    "    return df_input, nrows_ohe, ncols_ohe, float(best_score), time_exec, str(best_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-milton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle con la base de datos original\n",
    "pickle.dump(df,open(\"df_raw.pkl\",\"wb\"))\n",
    "df_clean, nrows_prev, ncols_prev, nrows_after, ncols_after, data_null_prev = cleaning(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-monitoring",
   "metadata": {},
   "source": [
    "**Extracción de datos del último día de 2020 hacia atrás**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = df_clean.loc[df_clean.inspection_date > pd.to_datetime('2020-12-31'), :].index\n",
    "df_clean.drop(var,axis=0).reset_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-arkansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_clean = pd.DataFrame({'nrows_prev' : nrows_prev,\n",
    "                        'ncols_prev' : ncols_prev,\n",
    "                        'nrows_after' : nrows_after,\n",
    "                        'ncols_after' : ncols_after,\n",
    "                        'data_null_prev' : data_null_prev}, index = [0])\n",
    "meta_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-silence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickles con la base de datos limpia y su metadata\n",
    "pickle.dump(df_clean,open(\"df_clean.pkl\",\"wb\"))\n",
    "pickle.dump(meta_clean,open(\"meta_clean.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe, nrows_ohe, ncols_ohe, best_score, time_exec, best_rf = feat_eng(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-italic",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_fe = pd.DataFrame({'nrows_ohe' : nrows_ohe,\n",
    "                        'ncols_ohe' : ncols_ohe,\n",
    "                        'best_score' : best_score,\n",
    "                        'time_exec' : time_exec,\n",
    "                        'best_rf' : best_rf}, index = [0])\n",
    "meta_fe                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-press",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickles con Feature Engineering y su metadata\n",
    "pickle.dump(df_fe,open(\"df_fe.pkl\",\"wb\"))\n",
    "pickle.dump(meta_fe,open(\"meta_fe.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pickle.load(open(\"df_raw.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pickle.load(open(\"df_clean.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-awareness",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "independent-coating",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "df_fe = pickle.load(open(\"df_fe.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-temple",
   "metadata": {},
   "source": [
    "**Transformación a OHE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faced-apparel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos OneHot Encoding\n",
    "data_input_ohe = pd.get_dummies(df_fe)\n",
    "etiqueta = data_input_ohe.label_results\n",
    "data_input_ohe= data_input_ohe.drop('label_results', axis = 1)\n",
    "variables_lista = list(data_input_ohe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "retired-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos TimeSeriesSplit para obtener las matrices de entrenamiento y prueba\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "for train_index, test_index in tscv.split(data_input_ohe):\n",
    "    X_train, X_test = data_input_ohe.iloc[train_index, :], data_input_ohe.iloc[test_index,:]\n",
    "    y_train, y_test = etiqueta.iloc[train_index], etiqueta.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "documentary-friend",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nrows_train</th>\n",
       "      <th>nrows_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>117340</td>\n",
       "      <td>39113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   nrows_train  nrows_test\n",
       "0       117340       39113"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metadata de las matrices para el modelo\n",
    "nrows_train = X_train.shape[0]\n",
    "nrows_test = X_test.shape[0]\n",
    "meta_train = pd.DataFrame({'nrows_train' : nrows_train,\n",
    "                           'nrows_test' : nrows_test}, index = [0])\n",
    "meta_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "romantic-celtic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156453, 528)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hacemos un solo DF para los Datasets de Entrenamiento y Prueba y con la etiqueta\n",
    "X_train_1 = X_train.assign(Set = 'entrenamiento')\n",
    "X_train_1 = X_train_1.assign(etiqueta = y_train)\n",
    "X_test_1 = X_test.assign(Set = 'prueba')\n",
    "X_test_1 = X_test_1.assign(etiqueta = y_test)\n",
    "df_train_test = pd.concat([X_train_1, X_test_1], axis = 0)\n",
    "\n",
    "df_train_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(df_train_test.etiqueta) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-match",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones para regresar el DataFrame con las etiquetas y los sets de entrenamiento y \n",
    "# y prueba a los cuatro DF Xtrain, Ytrain, Xtst y Ytest\n",
    "X_train_2 = df_train_test[df_train_test.Set == 'entrenamiento']\n",
    "y_train_2 = X_train_2.etiqueta\n",
    "X_train_2 = X_train_2.iloc[:,0:df_train_test.shape[1]-2]\n",
    "\n",
    "\n",
    "X_test_2 = df_train_test[df_train_test.Set == 'prueba']\n",
    "y_test_2 = X_test_2.etiqueta\n",
    "X_test_2 = X_test_2.iloc[:,0:df_train_test.shape[1]-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-florist",
   "metadata": {},
   "source": [
    "## Selección de Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adopted-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmos a evaluar: DecisionTree y RandomForest\n",
    "algorithms_dict = {'tree': 'tree_grid_search'}\n",
    "algorithms = ['tree']\n",
    "# Hiperparámetros a evaluar en cada algoritmo:\n",
    "grid_search_dict = {'tree_grid_search': {'max_depth': [5,10,15], \n",
    "                                         'min_samples_leaf': [3,5,7]}}\n",
    "\n",
    "# Configuraciones generales de cada algoritmo a evaluar:\n",
    "estimators_dict = {'tree': DecisionTreeClassifier(random_state=1111)}\n",
    "best_estimators = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "large-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for algorithm in algorithms:\n",
    "    estimator = estimators_dict[algorithm]\n",
    "    grid_search_to_look = algorithms_dict[algorithm]\n",
    "    grid_params = grid_search_dict[grid_search_to_look]\n",
    "    gs = GridSearchCV(estimator, grid_params, scoring='precision', cv=tscv, n_jobs=-1)\n",
    "    start_time = time.time()\n",
    "    #train\n",
    "    gs.fit(X_train, y_train)\n",
    "    #best estimator\n",
    "    best_estimators.append(gs)\n",
    "time_exec = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "played-scanning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=15, min_samples_leaf=5, random_state=1111)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_tree = best_estimators[0].best_estimator_\n",
    "best_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "transsexual-feedback",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.768035</td>\n",
       "      <td>1.826298</td>\n",
       "      <td>0.152156</td>\n",
       "      <td>0.031171</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_leaf': 5}</td>\n",
       "      <td>0.758883</td>\n",
       "      <td>0.773153</td>\n",
       "      <td>0.756926</td>\n",
       "      <td>0.762987</td>\n",
       "      <td>0.007233</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.313803</td>\n",
       "      <td>1.386620</td>\n",
       "      <td>0.141566</td>\n",
       "      <td>0.013434</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>{'max_depth': 15, 'min_samples_leaf': 7}</td>\n",
       "      <td>0.759076</td>\n",
       "      <td>0.772870</td>\n",
       "      <td>0.756963</td>\n",
       "      <td>0.762970</td>\n",
       "      <td>0.007054</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "7       3.768035      1.826298         0.152156        0.031171   \n",
       "8       3.313803      1.386620         0.141566        0.013434   \n",
       "\n",
       "  param_max_depth param_min_samples_leaf  \\\n",
       "7              15                      5   \n",
       "8              15                      7   \n",
       "\n",
       "                                     params  split0_test_score  \\\n",
       "7  {'max_depth': 15, 'min_samples_leaf': 5}           0.758883   \n",
       "8  {'max_depth': 15, 'min_samples_leaf': 7}           0.759076   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "7           0.773153           0.756926         0.762987        0.007233   \n",
       "8           0.772870           0.756963         0.762970        0.007054   \n",
       "\n",
       "   rank_test_score  \n",
       "7                1  \n",
       "8                2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = pd.DataFrame(best_estimators[0].cv_results_)\n",
    "r = r.sort_values(\"rank_test_score\")\n",
    "r.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "rough-canadian",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = r.params.astype(str)\n",
    "test_mod = \"|\".join(lista)\n",
    "lista_2 = r.mean_test_score.astype(str)\n",
    "mean_scores = \"|\".join(lista_2)\n",
    "lista_3 = r.rank_test_score.astype(str)\n",
    "rank_model = \"|\".join(lista_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sunset-railway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'max_depth': 15, 'min_samples_leaf': 5}|{'max_depth': 15, 'min_samples_leaf': 7}|{'max_depth': 15, 'min_samples_leaf': 3}|{'max_depth': 10, 'min_samples_leaf': 7}|{'max_depth': 10, 'min_samples_leaf': 3}|{'max_depth': 10, 'min_samples_leaf': 5}|{'max_depth': 5, 'min_samples_leaf': 7}|{'max_depth': 5, 'min_samples_leaf': 5}|{'max_depth': 5, 'min_samples_leaf': 3}\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "incorporate-eclipse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.762987478396355|0.7629698087544893|0.7627601253893648|0.7625208491614459|0.7625102065289221|0.7625073151753209|0.7624475294845636|0.7624157249220945|0.7623866831814622'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "featured-empty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.762987478396355|0.7629698087544893|0.7627601253893648|0.7625208491614459|0.7625102065289221|0.7625073151753209|0.7624475294845636|0.7624157249220945|0.7623866831814622'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model.fit(X_train, y_train)\n",
    "pickle.dump(m, open(\"best_model.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pickle.load(open(\"best_model.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = m.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(resultados)\n",
    "res=res.rename(columns = {0:'label'})\n",
    "res.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-russian",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_2 = pd.DataFrame(y_test)\n",
    "res_2.label_results.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-blame",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 < time_exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-sunday",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-divorce",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.DataFrame(grid_search_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {'t_exec':time_exec,'best_tree':best_tree}\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame(dic,index=[0])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-agriculture",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.t_exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-amateur",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-brand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmos a evaluar: DecisionTree y RandomForest\n",
    "algorithms_dict = {'random_forest': 'rf_grid_search'}\n",
    "algorithms = ['random_forest']\n",
    "# Hiperparámetros a evaluar en cada algoritmo:\n",
    "grid_search_dict = {'rf_grid_search': {'n_estimators': [500],  \n",
    "                                      'max_depth': [5,10], \n",
    "                                      'min_samples_leaf': [10]}}\n",
    "\n",
    "# Configuraciones generales de cada algoritmo a evaluar:\n",
    "estimators_dict = {'random_forest': RandomForestClassifier(oob_score=True, random_state=2222)}\n",
    "best_estimators = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for algorithm in algorithms:\n",
    "    estimator = estimators_dict[algorithm]\n",
    "    grid_search_to_look = algorithms_dict[algorithm]\n",
    "    grid_params = grid_search_dict[grid_search_to_look]\n",
    "    gs = GridSearchCV(estimator, grid_params, scoring='precision', cv=tscv, n_jobs=-1)\n",
    "    start_time = time.time()\n",
    "    #train\n",
    "    gs.fit(X_train, y_train)\n",
    "    #best estimator\n",
    "    best_estimators.append(gs)\n",
    "time_exec = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = best_estimators[0].best_estimator_\n",
    "best_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = best_rf\n",
    "m_rf = model_rf.fit(X_train, y_train)\n",
    "pickle.dump(m, open(\"best_model_rf.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_rf = best_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_rf = pd.DataFrame(resultados_rf)\n",
    "res_rf = res_rf.rename(columns = {0:'label'})\n",
    "res_rf.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-departure",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
