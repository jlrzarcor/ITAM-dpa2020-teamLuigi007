{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib.pyplot import figure\n",
    "from sodapy import Socrata\n",
    "import seaborn as sns \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, KBinsDiscretizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aequitas.group import Group\n",
    "from aequitas.bias import Bias\n",
    "from aequitas.fairness import Fairness\n",
    "from aequitas.plotting import Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*Conexión a la API Chicago Food Insp*\n",
    "#client = Socrata(\"data.cityofchicago.org\", None)\n",
    "#results = client.get(\"4ijn-s7e5\", limit=400000)\n",
    "#df = pd.DataFrame.from_records(results)\n",
    "#col_names = df.columns.to_list()\n",
    "#col_name = []\n",
    "#for i in range(len(col_names)):\n",
    "#    col_name.append(col_names[i].replace(\" \", \"_\").lower())\n",
    "#df.columns =col_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza y preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standarize_column_strings(df, columns, excluded_punctuation=\".,*¿?¡!\"):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(\" \", \"_\")\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(\"-\", \"_\")\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(\"á\", \"a\")\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(\"é\", \"e\")\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(\"í\", \"i\")\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(\"ó\", \"o\")\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(\"ú\", \"u\")\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(\"ü\", \"u\")\n",
    "        df[col] = df[col].str.lower().astype(str).str.replace(r\"[^a-zA-Z\\d\\_]+\", \"\")\n",
    "        for ch in excluded_punctuation:\n",
    "            df[col] = df[col].str.replace(ch, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(df):\n",
    "    '''\n",
    "    Función que convierte las columnas del Data Frame al tipo y forma que se necesita para\n",
    "    los análisis posteriores\n",
    "    \n",
    "    inputs: Data Frame almacenado en el S3 (ingesta.pkl)\n",
    "    outputs: Data Frame con las variables en formato adecuado (df_clean.pkl)\n",
    "        \n",
    "    '''\n",
    "    #df = pickle.load(open(\"ingesta.pkl\",\"rb\"))\n",
    "    nrows_prev = df.shape[0]\n",
    "    ncols_prev = df.shape[1]\n",
    "    data_null_prev = df.isnull().sum().sum()\n",
    "    # Variables de texto\n",
    "    df['violations']= df['violations'].astype('object')\n",
    "    df['violations_count'] = df.violations.str.count(r'\\|')+1\n",
    "    df['violations_count'] = df.violations_count.fillna(0)\n",
    "    df['violations_count'] = df['violations_count'].astype('int')\n",
    "    # Variables categóricas\n",
    "    df['dba_name']= df['dba_name'].astype('object')\n",
    "    df['aka_name']= df['aka_name'].astype('object')\n",
    "    df['facility_type']= df['facility_type'].astype('category')\n",
    "    df['risk']= df['risk'].astype('category')\n",
    "    df['address']= df['address'].astype('category')\n",
    "    df['city']= df['city'].astype('category')\n",
    "    df['state']= df['state'].astype('category')\n",
    "    df['inspection_type']= df['inspection_type'].astype('category')\n",
    "    df['results']= df['results'].astype('category')\n",
    "    # Variable label_risk\n",
    "    df['risk'] = df['risk'].replace([\"Risk 1 (High)\"],3)\n",
    "    df['risk'] = df['risk'].replace([\"Risk 2 (Medium)\"],2)\n",
    "    df['risk'] = df['risk'].replace([\"Risk 3 (Low)\"],1)\n",
    "    df['risk'] = df['risk'].replace([\"All\"],0)\n",
    "    df['risk'] = pd.to_numeric(df['risk'], errors='coerce')\n",
    "    df=df.rename(columns = {'risk':'label_risk'})\n",
    "    df['label_risk'] = df['label_risk'].fillna(3)\n",
    "    df['label_risk'] = df['label_risk'].astype('int')\n",
    "    # Variables de fecha\n",
    "    df['inspection_date'] = pd.to_datetime(df['inspection_date'], infer_datetime_format=True)\n",
    "    df['inspection_month']=df['inspection_date'].dt.month\n",
    "    MONTH = 12\n",
    "    df['sin_mnth'] = np.sin(2*np.pi*df.inspection_month/MONTH)\n",
    "    df['cos_mnth'] = np.cos(2*np.pi*df.inspection_month/MONTH)\n",
    "    df['inspection_weekday']=df['inspection_date'].dt.weekday\n",
    "    WEEKDAY = 7\n",
    "    df['sin_wkd'] = np.sin(2*np.pi*df.inspection_weekday/WEEKDAY)\n",
    "    df['cos_wkd'] = np.cos(2*np.pi*df.inspection_weekday/WEEKDAY)\n",
    "    # Etiqueta\n",
    "    df['label_results'] = df['results'].apply(lambda x: int(0) if x == 'Fail' else (int(1) if x in ['Pass','Pass w/Conditions'] else int(2)))\n",
    "    # Imputación de datos\n",
    "    df.drop(['violations'],axis = 1, inplace = True)\n",
    "    df.drop(['results'], axis = 1, inplace = True)\n",
    "    df.drop(df.loc[df['license_'].isnull()].index, inplace=True)\n",
    "    df.drop(df.loc[df['zip'].isnull()].index, inplace=True)\n",
    "    df.drop(df.loc[df['label_results'] == 2].index, inplace=True)\n",
    "    df['aka_name'] = df['aka_name'].fillna(df['dba_name'])\n",
    "    df['dba_name']= df['dba_name'].astype(str).str.lower()\n",
    "    df['aka_name']= df['aka_name'].astype(str).str.lower()\n",
    "    df['facility_type']= df['facility_type'].astype(str).str.lower()\n",
    "    df['state']= df['state'].astype(str).str.lower()\n",
    "    df['inspection_type']= df['inspection_type'].astype(str).str.lower()\n",
    "    df = df[~df['state'].isin(['wi', 'ny', 'in'])]\n",
    "    col_text = ['dba_name','aka_name']\n",
    "    df.rename(columns={'license_':'license'}, inplace=True)\n",
    "    standarize_column_strings(df, col_text)\n",
    "    df_dict_dummy = pd.DataFrame(df['aka_name'])\n",
    "    df_dict_dummy['facility_type'] = df['facility_type']\n",
    "    df_dict_dummy.drop(df_dict_dummy.loc[df_dict_dummy['facility_type'].isnull()].index, inplace=True)\n",
    "    group = df_dict_dummy.groupby('aka_name')\n",
    "    df_dict_dummy2 = group.apply(lambda x: x['facility_type'].unique())\n",
    "    df_dict_dummy3 = df_dict_dummy2.to_frame()\n",
    "    df_dict_dummy3.reset_index(level = 'aka_name', inplace = True)\n",
    "    df_dict_dummy3 = df_dict_dummy3.rename(columns = {0:'facility_type'})\n",
    "    df_dict_dummy3['facility_type'] = df_dict_dummy3['facility_type'].apply(lambda x: str(x[0]))\n",
    "    df2 = pd.merge(df,df_dict_dummy3, how = 'left', on = 'aka_name')\n",
    "    df2['facility_type_x'] = df2['facility_type_x'].fillna(df2['facility_type_y'])\n",
    "    df2['facility_type_x'] = df2['facility_type_x'].fillna('restaurant')\n",
    "    df2=df2.rename(columns = {'facility_type_x':'facility_type'})\n",
    "    df2.drop(['inspection_id','dba_name','address','city','state','latitude','longitude','location','facility_type_y','inspection_weekday','inspection_month'],axis = 1, inplace = True)\n",
    "    df2 = df2.dropna() #---------------------------------------->adición\n",
    "    nrows_after = df2.shape[0]\n",
    "    ncols_after = df2.shape[1]\n",
    "    return df2, nrows_prev, ncols_prev, nrows_after, ncols_after, data_null_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zips(x,lev,dic):\n",
    "    if x in lev.zip.to_list():\n",
    "        return dic[x]\n",
    "    else:\n",
    "        return 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_eng2(df_fe):\n",
    "    '''\n",
    "    Función que realiza la selección de los features que serán utilizdos para la clasificación\n",
    "    \n",
    "    inputs: Data Frame limpio (df_clean.pkl)\n",
    "    outputs: Data Frame con la matriz de diseño para el modelo (df_clean.pkl)\n",
    "        \n",
    "    '''\n",
    "    # Transformación de variables facility_type y zip\n",
    "    tipo = pd.DataFrame(df_fe.facility_type.value_counts())\n",
    "    tipo['name'] = tipo.index\n",
    "    tipo.index = range(len(tipo.name))\n",
    "    grupo1 = tipo.iloc[0:4,1].tolist()\n",
    "    grupo2 = tipo.iloc[[5,6,7,11],1].tolist()\n",
    "    df_fe['class'] = df_fe['facility_type'].apply(lambda x: x if x in grupo1 else ('daycare' if x in grupo2 else 'other'))\n",
    "    lev = pd.read_csv('levels.csv')\n",
    "    lev['zip'] = lev['zip'].astype(str)\n",
    "    lev.index = lev.zip\n",
    "    dic = lev.level.to_dict()\n",
    "    df_fe['level'] = df_fe['zip'].apply(lambda x: zips(x,lev,dic))\n",
    "    # Transformación a OHE\n",
    "    df_fe = df_fe.sort_values(by='inspection_date', ascending=True)\n",
    "    df_input = pd.DataFrame(df_fe[['label_risk','label_results','level','class']])\n",
    "    data_input_ohe = pd.get_dummies(df_input)\n",
    "    etiqueta = data_input_ohe.label_results\n",
    "    data_input_ohe= data_input_ohe.drop('label_results', axis = 1)\n",
    "    variables_lista = list(data_input_ohe.columns)\n",
    "    # Grid Search\n",
    "    np.random.seed(20201124)\n",
    "    # ocuparemos un RF\n",
    "    classifier = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=1234)\n",
    "    # separando en train, test\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(data_input_ohe, etiqueta, test_size=0.3)\n",
    "\n",
    "    # definicion de los hiperparametros que queremos probar\n",
    "    hyper_param_grid = {'n_estimators': [300, 400], #'min_samples_leaf': [3,5,7],\n",
    "                        'max_depth': [7, 10],\n",
    "                        #'min_samples_split': [3],\n",
    "                        'max_features': [5, 10],\n",
    "                        'criterion': ['gini']}\n",
    "    # usamos TimeSeriesSplit para dividir respetando el orden cronológico\n",
    "    tscv = TimeSeriesSplit(n_splits=6)\n",
    "    # This was the trickiest part as a newbie. Straight from the docs\n",
    "    # If you only have experience with CV splits this way\n",
    "    # of making the splits might seem foreign. Fret not.\n",
    "    for train_index, test_index in tscv.split(data_input_ohe):\n",
    "        X_train, X_test = data_input_ohe.iloc[train_index, :], data_input_ohe.iloc[test_index,:]\n",
    "        y_train, y_test = etiqueta.iloc[train_index], etiqueta.iloc[test_index]\n",
    "    # ocupemos grid search\n",
    "    gs = GridSearchCV(classifier, \n",
    "                           hyper_param_grid, \n",
    "                           scoring = 'precision', return_train_score=True,\n",
    "                           cv = tscv)\n",
    "    start_time = time.time()\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_rf = gs.best_estimator_\n",
    "    best_score = gs.best_estimator_.oob_score_\n",
    "    feature_importance = pd.DataFrame({'importance':\\\n",
    "                                       best_rf.feature_importances_,\\\n",
    "                                       'feature': variables_lista})\n",
    "    feature_importance=feature_importance.sort_values(by=\"importance\", ascending=False)\n",
    "    #fi_out = feature_importance.head(10)\n",
    "    time_exec = time.time() - start_time\n",
    "    nrows_ohe = data_input_ohe.shape[0]\n",
    "    ncols_ohe = data_input_ohe.shape[1]\n",
    "    #print(\"Tiempo en ejecutar: \", time.time() - start_time)\n",
    "    return df_input, nrows_ohe, ncols_ohe, float(best_score), time_exec, str(best_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feat_eng(df_fe):\n",
    "    '''\n",
    "    Función que realiza la selección de los features que serán utilizdos para la clasificación\n",
    "    \n",
    "    inputs: Data Frame limpio (df_clean.pkl)\n",
    "    outputs: Data Frame con la matriz de diseño para el modelo (df_clean.pkl)\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Transformación de variables facility_type y zip\n",
    "    tipo = pd.DataFrame(df_fe.facility_type.value_counts())\n",
    "    tipo['name'] = tipo.index\n",
    "    tipo.index = range(len(tipo.name))\n",
    "    grupo1 = tipo.iloc[0:4,1].tolist()\n",
    "    grupo2 = tipo.iloc[[5,6,7,11],1].tolist()\n",
    "    df_fe['class'] = df_fe['facility_type'].apply(lambda x: x if x in grupo1 else ('daycare' if x in grupo2 else 'other'))\n",
    "    lev = pd.read_csv('zip_catalog.csv')\n",
    "    lev['zip'] = lev['zip'].astype(str)\n",
    "    lev.index = lev.zip\n",
    "    dic = lev.level.to_dict()\n",
    "    df_fe['level'] = df_fe['zip'].apply(lambda x: zips(x,lev,dic))\n",
    "    \n",
    "    \n",
    "    # Transformación a OHE\n",
    "    df_fe = df_fe.sort_values(by='inspection_date', ascending=True)\n",
    "    df_input = pd.DataFrame(df_fe[['label_risk','label_results','level','class']])\n",
    "    data_input_ohe = pd.get_dummies(df_input)\n",
    "    etiqueta = data_input_ohe.label_results\n",
    "    data_input_ohe= data_input_ohe.drop('label_results', axis = 1)\n",
    "    variables_lista = list(data_input_ohe.columns)\n",
    "    # Grid Search\n",
    "    np.random.seed(20201124)\n",
    "    # ocuparemos un RF\n",
    "    classifier = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=1234)\n",
    "    # separando en train, test\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(data_input_ohe, etiqueta, test_size=0.3)\n",
    "\n",
    "    # definicion de los hiperparametros que queremos probar\n",
    "    hyper_param_grid = {'n_estimators': [300, 400], #'min_samples_leaf': [3,5,7],\n",
    "                        'max_depth': [7, 10],\n",
    "                        #'min_samples_split': [3],\n",
    "                        'max_features': [3, 5, 6],\n",
    "                        'criterion': ['gini']}\n",
    "    # usamos TimeSeriesSplit para dividir respetando el orden cronológico\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    # This was the trickiest part as a newbie. Straight from the docs\n",
    "    # If you only have experience with CV splits this way\n",
    "    # of making the splits might seem foreign. Fret not.\n",
    "    for train_index, test_index in tscv.split(data_input_ohe):\n",
    "        X_train, X_test = data_input_ohe.iloc[train_index, :], data_input_ohe.iloc[test_index,:]\n",
    "        y_train, y_test = etiqueta.iloc[train_index], etiqueta.iloc[test_index]\n",
    "    # ocupemos grid search\n",
    "    gs = GridSearchCV(classifier, \n",
    "                           hyper_param_grid, \n",
    "                           scoring = 'precision', return_train_score=True,\n",
    "                           cv = tscv)\n",
    "    start_time = time.time()\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_rf = gs.best_estimator_\n",
    "    best_score = gs.best_estimator_.oob_score_\n",
    "    feature_importance = pd.DataFrame({'importance':\\\n",
    "                                       best_rf.feature_importances_,\\\n",
    "                                       'feature': variables_lista})\n",
    "    feature_importance=feature_importance.sort_values(by=\"importance\", ascending=False)\n",
    "    \n",
    "    #fi_out = feature_importance.head(10)\n",
    "    \n",
    "    time_exec = time.time() - start_time\n",
    "    nrows_ohe = data_input_ohe.shape[0]\n",
    "    ncols_ohe = data_input_ohe.shape[1]\n",
    "    \n",
    "    #print(\"Tiempo en ejecutar: \", time.time() - start_time)\n",
    "    \n",
    "    return df_input, nrows_ohe, ncols_ohe, float(best_score), time_exec, str(best_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df_fe):\n",
    "    # Aplicamos OneHot Encoding\n",
    "    data_input_ohe = pd.get_dummies(df_fe)\n",
    "    etiqueta = data_input_ohe.label_results\n",
    "    data_input_ohe= data_input_ohe.drop('label_results', axis = 1)\n",
    "    variables_lista = list(data_input_ohe.columns)\n",
    "    # Hacemos TimeSeriesSplit para obtener las matrices de entrenamiento y prueba\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    for train_index, test_index in tscv.split(data_input_ohe):\n",
    "        X_train, X_test = data_input_ohe.iloc[train_index, :], data_input_ohe.iloc[test_index,:]\n",
    "        y_train, y_test = etiqueta.iloc[train_index], etiqueta.iloc[test_index]\n",
    "    # Metadata de las matrices para el modelo\n",
    "    nrows_train = X_train.shape[0]\n",
    "    nrows_test = X_test.shape[0]\n",
    "    meta_train = pd.DataFrame({'nrows_train' : nrows_train,\n",
    "                           'nrows_test' : nrows_test}, index = [0])\n",
    "    # Hacemos un solo DF para los Datasets de Entrenamiento y Prueba y con la etiqueta\n",
    "    X_train_1 = X_train.assign(Set = 'entrenamiento')\n",
    "    X_train_1 = X_train_1.assign(etiqueta = y_train)\n",
    "    X_test_1 = X_test.assign(Set = 'prueba')\n",
    "    X_test_1 = X_test_1.assign(etiqueta = y_test)\n",
    "    df_train_test = pd.concat([X_train_1, X_test_1], axis = 0)\n",
    "    return df_train_test, nrows_train, nrows_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(df_train_test):\n",
    "    # Funciones para regresar el DataFrame con las etiquetas y los sets de entrenamiento y \n",
    "    # y prueba a los cuatro DF X_train, Y_train, X_test y Y_test\n",
    "    X_train = df_train_test[df_train_test.Set == 'entrenamiento']\n",
    "    y_train = X_train.etiqueta\n",
    "    X_train = X_train.iloc[:,0:df_train_test.shape[1]-2]\n",
    "    X_test = df_train_test[df_train_test.Set == 'prueba']\n",
    "    #y_test = X_test.etiqueta\n",
    "    X_test = X_test.iloc[:,0:df_train_test.shape[1]-2]\n",
    "    # Algoritmos a evaluar: DecisionTree y RandomForest\n",
    "    algorithms_dict = {'tree': 'tree_grid_search'}\n",
    "    algorithms = ['tree']\n",
    "    # Hiperparámetros a evaluar en cada algoritmo:\n",
    "    grid_search_dict = {'tree_grid_search': {'max_depth': [5,10,15], \n",
    "                                         'min_samples_leaf': [3,5,7]}}\n",
    "    # Configuraciones generales de cada algoritmo a evaluar:\n",
    "    estimators_dict = {'tree': DecisionTreeClassifier(random_state=1111)}\n",
    "    best_estimators = []\n",
    "    # Magic loop\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    start_time = time.time()\n",
    "    for algorithm in algorithms:\n",
    "        estimator = estimators_dict[algorithm]\n",
    "        grid_search_to_look = algorithms_dict[algorithm]\n",
    "        grid_params = grid_search_dict[grid_search_to_look]\n",
    "        gs = GridSearchCV(estimator, grid_params, scoring='precision', cv=tscv, n_jobs=-1)\n",
    "        #train\n",
    "        gs.fit(X_train, y_train)\n",
    "        #best estimator\n",
    "        best_estimators.append(gs)\n",
    "    \n",
    "    # MEtadata:\n",
    "    # Mejor modelo de árbol\n",
    "    best_tree = best_estimators[0].best_estimator_\n",
    "    # Tiempo de ejecución\n",
    "    t_exec = time.time() - start_time\n",
    "    # Información de los modelos considerados en la selección\n",
    "    r = pd.DataFrame(best_estimators[0].cv_results_)\n",
    "    r = r.sort_values(\"rank_test_score\")\n",
    "    lista = r.params.astype(str)\n",
    "    test_mod = \"|\".join(lista)\n",
    "    lista_2 = r.mean_test_score.astype(str)\n",
    "    mean_scores = \"|\".join(lista_2)\n",
    "    lista_3 = r.rank_test_score.astype(str)\n",
    "    rank_model = \"|\".join(lista_3)\n",
    "    # Persistir mejor modelo en .pkl\n",
    "    obj_model = best_tree.fit(X_train, y_train)\n",
    "#    pickle.dump(best_tree, open(\"best_model.pkl\", 'wb'))\n",
    "#    model_pkl = pickle.load(open(\"best_model.pkl\",\"rb\"))\n",
    "    return obj_model, str(best_tree), t_exec, test_mod, mean_scores, rank_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sesgo e Inequidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_fair(modelo,df_train_test,df_fe):\n",
    "    # Separamos los sets de entrenamiento y prueba\n",
    "    # Entrenamiento\n",
    "    X_train = df_train_test[df_train_test.Set == 'entrenamiento']\n",
    "    y_train = X_train.etiqueta\n",
    "    X_train = X_train.iloc[:,0:df_train_test.shape[1]-2]\n",
    "    # Prueba\n",
    "    X_test = df_train_test[df_train_test.Set == 'prueba']\n",
    "    y_test = X_test.etiqueta\n",
    "    X_test = X_test.iloc[:,0:df_train_test.shape[1]-2]\n",
    "    predicted_scores = modelo.predict_proba(X_test)\n",
    "    \n",
    "    # Se conforma el DataFrame que necesita Aequitas para el sesgo e inequidad de la variable facility_type (class)\n",
    "    y_test2 = y_test.reset_index(drop=True)\n",
    "    \n",
    "    df_dummy = pd.DataFrame()\n",
    "    df_dummy['scores'] = pd.Series(predicted_scores[:,1])\n",
    "    df_dummy['predic'] = np.where(df_dummy['scores'] < 0.7,0,1)\n",
    "       \n",
    "    df_aeq = pd.DataFrame()\n",
    "    df_aeq['real'] = y_test2\n",
    "    df_aeq['prediccion'] = df_dummy.predic\n",
    "    df_aeq['faciliy_type'] = df_fe['class'].tail(len(df_dummy.predic)).reset_index(drop=True)\n",
    "        \n",
    "    # Asignamos nuevos índices y los nombres de las columnas para que los reconozca la función\n",
    "    df_aeq = df_aeq.reset_index(drop=True)\n",
    "    df_aeq.columns = ['label_value','score','class']\n",
    "    # Se obtienen las métricas\n",
    "    g = Group()\n",
    "    xtab, attrbs = g.get_crosstabs(df_aeq)\n",
    "    absolute_metrics = g.list_absolute_metrics(xtab)\n",
    "    metrics1 = xtab[['attribute_name', 'attribute_value']+[col for col in xtab.columns if col in absolute_metrics]].round(2)\n",
    "    \n",
    "    # Se conforma el DataFrame que necesita Aequitas para el sesgo e inequidad de la variable zip (level)\n",
    "    df_aeq2 = pd.DataFrame()\n",
    "    df_aeq2['real'] = y_test2\n",
    "    df_aeq2['prediccion'] = df_dummy['predic']\n",
    "    df_aeq2['zip'] = df_fe['level'].tail(len(df_dummy.predic)).reset_index(drop=True)\n",
    "    # Asignamos nuevos índices y los nombres de las columnas para que los reconozca la función\n",
    "    df_aeq2 = df_aeq2.reset_index(drop=True)\n",
    "    df_aeq2.columns = ['label_value','score','level']\n",
    "    # Se obtienen las métricas\n",
    "    g2 = Group()\n",
    "    xtab2, attrbs2 = g2.get_crosstabs(df_aeq2)\n",
    "    absolute_metrics2 = g2.list_absolute_metrics(xtab2)\n",
    "    metrics2 = xtab2[['attribute_name', 'attribute_value']+[col for col in xtab2.columns if col in absolute_metrics2]].round(2)\n",
    "    \n",
    "    df_labels = pd.DataFrame()\n",
    "    df_labels['scores'] = pd.Series(predicted_scores[:,1])\n",
    "    df_labels['predicted'] = np.where(df_dummy['scores'] < 0.7,0,1)\n",
    "    df_labels['label'] = y_test2\n",
    "    \n",
    "    metrics = pd.concat([metrics1,metrics2]).reset_index(drop = True)\n",
    "    metrics = metrics.fillna(0)\n",
    "        \n",
    "    # Metadata\n",
    "    n_groups = len(metrics1.attribute_value) + len(metrics2.attribute_value)\n",
    "    n_attribute = metrics.attribute_name.nunique()\n",
    "    prop_pos_pred = df_labels.predicted.sum()/len(df_labels.predicted)\n",
    "    prop_pos_real = df_labels.label.sum()/len(df_labels.label)\n",
    "        \n",
    "    return df_labels, metrics, n_groups, n_attribute, prop_pos_pred, prop_pos_real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Frame original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pickle.load(open(\"consecutive_raw.pkl\",\"rb\"))\n",
    "df_raw = pd.DataFrame(df_raw)\n",
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2, nrows_prev, ncols_prev, nrows_after, ncols_after, data_null_prev = cleaning(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input, nrows_ohe, ncols_ohe, best_score, time_exec, best_rf = feat_eng(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.421745538711548"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input.label_risk.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input.label_results.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_test, nrows_train, nrows_test = train(df_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_model, best_tree, t_exec, test_mod, mean_scores, rank_model = model(df_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels, metrics, n_groups, n_attribute, prop_pos_pred, prop_pos_real = bias_fair(obj_model,df_train_test,df_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attribute_name</th>\n",
       "      <th>attribute_value</th>\n",
       "      <th>tpr</th>\n",
       "      <th>tnr</th>\n",
       "      <th>for</th>\n",
       "      <th>fdr</th>\n",
       "      <th>fpr</th>\n",
       "      <th>fnr</th>\n",
       "      <th>npv</th>\n",
       "      <th>precision</th>\n",
       "      <th>ppr</th>\n",
       "      <th>pprev</th>\n",
       "      <th>prev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>class</td>\n",
       "      <td>bakery</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>class</td>\n",
       "      <td>daycare</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>class</td>\n",
       "      <td>grocery store</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>class</td>\n",
       "      <td>other</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>class</td>\n",
       "      <td>restaurant</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>class</td>\n",
       "      <td>school</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>level</td>\n",
       "      <td>downtown</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>level</td>\n",
       "      <td>high</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>level</td>\n",
       "      <td>low-mid</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>level</td>\n",
       "      <td>other</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  attribute_name attribute_value   tpr   tnr   for   fdr   fpr   fnr   npv  \\\n",
       "0          class          bakery  0.00  1.00  0.50  0.00  0.00  1.00  0.50   \n",
       "1          class         daycare  1.00  0.33  0.00  0.33  0.67  0.00  1.00   \n",
       "2          class   grocery store  0.48  0.53  0.58  0.41  0.47  0.52  0.42   \n",
       "3          class           other  0.00  1.00  0.70  0.00  0.00  1.00  0.30   \n",
       "4          class      restaurant  1.00  0.00  0.00  0.18  1.00  0.00  0.00   \n",
       "5          class          school  0.94  0.20  0.50  0.21  0.80  0.06  0.50   \n",
       "6          level        downtown  0.97  0.00  1.00  0.12  1.00  0.03  0.00   \n",
       "7          level            high  0.87  0.30  0.58  0.21  0.70  0.13  0.42   \n",
       "8          level         low-mid  0.83  0.35  0.53  0.25  0.65  0.17  0.47   \n",
       "9          level           other  0.75  0.00  1.00  0.25  1.00  0.25  0.00   \n",
       "\n",
       "   precision   ppr  pprev  prev  \n",
       "0       0.00  0.00   0.00  0.50  \n",
       "1       0.67  0.03   0.86  0.57  \n",
       "2       0.59  0.09   0.47  0.58  \n",
       "3       0.00  0.00   0.00  0.70  \n",
       "4       0.82  0.77   1.00  0.82  \n",
       "5       0.79  0.11   0.90  0.76  \n",
       "6       0.88  0.18   0.97  0.88  \n",
       "7       0.79  0.51   0.83  0.75  \n",
       "8       0.75  0.28   0.77  0.70  \n",
       "9       0.75  0.02   0.80  0.80  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "etiqueta = etiqueta.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real</th>\n",
       "      <th>prediccion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>215 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     real  prediccion\n",
       "0       1           1\n",
       "1       0           0\n",
       "2       0           1\n",
       "3       1           1\n",
       "4       1           1\n",
       "..    ...         ...\n",
       "210     1           1\n",
       "211     1           1\n",
       "212     1           1\n",
       "213     1           1\n",
       "214     1           1\n",
       "\n",
       "[215 rows x 2 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aeq = pd.DataFrame()\n",
    "df_aeq['real'] = etiqueta\n",
    "df_aeq['prediccion'] = debug.predic\n",
    "df_aeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(df2,open(\"limpieza.pkl\",\"wb\"))             # DF limpio\n",
    "pickle.dump(df_input,open(\"feat_eng.pkl\",\"wb\"))        # DF con el Feature Engineering\n",
    "pickle.dump(df_train_test,open(\"train_test.pkl\",\"wb\")) # Entrenamiento y Prueba\n",
    "pickle.dump(obj_model,open(\"obj_model.pkl\",\"wb\"))      # Selección del Modelo\n",
    "pickle.dump(metrics,open(\"metrics.pkl\",\"wb\"))          # Métricas de desempeño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pickle.load(open(\"df_raw.pkl\",\"rb\"))\n",
    "df_clean = pickle.load(open(\"df_clean.pkl\",\"rb\"))\n",
    "df_fe = pickle.load(open(\"df_fe.pkl\",\"rb\"))\n",
    "m = pickle.load(open(\"best_model.pkl\",\"rb\"))\n",
    "df_trn_tst = pickle.load(open(\"df_trn_tst.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformación a OHE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle con la base de datos original\n",
    "#pickle.dump(df,open(\"df_raw.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpia la Base de Datos\n",
    "#df_raw = pickle.load(open(\"df_raw.pkl\",\"rb\"))\n",
    "#df_clean, nrows_prev, ncols_prev, nrows_after, ncols_after, data_null_prev = cleaning(df_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracción de datos del último día de 2020 hacia atrás**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pickle.load(open(\"df_clean.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = df_clean.loc[df_clean.inspection_date > pd.to_datetime('2020-12-31'), :].index\n",
    "df_clean = df_clean.drop(var,axis=0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta_clean = pd.DataFrame({'nrows_prev' : nrows_prev,\n",
    "#                        'ncols_prev' : ncols_prev,\n",
    "#                        'nrows_after' : nrows_after,\n",
    "#                        'ncols_after' : ncols_after,\n",
    "#                        'data_null_prev' : data_null_prev}, index = [0])\n",
    "#meta_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickles con la base de datos limpia y su metadata\n",
    "pickle.dump(df_clean,open(\"df_clean.pkl\",\"wb\"))\n",
    "#pickle.dump(meta_clean,open(\"meta_clean.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_fe, nrows_ohe, ncols_ohe, best_score, time_exec, best_rf = feat_eng(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_fe = pd.DataFrame({'nrows_ohe' : nrows_ohe,\n",
    "                        'ncols_ohe' : ncols_ohe,\n",
    "                        'best_score' : best_score,\n",
    "                        'time_exec' : time_exec,\n",
    "                        'best_rf' : best_rf}, index = [0])\n",
    "meta_fe                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickles con Feature Engineering y su metadata\n",
    "pickle.dump(df_fe,open(\"df_fe.pkl\",\"wb\"))\n",
    "pickle.dump(meta_fe,open(\"meta_fe.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "df_fe = pickle.load(open(\"df_fe.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos OneHot Encoding\n",
    "data_input_ohe = pd.get_dummies(df_fe)\n",
    "etiqueta = data_input_ohe.label_results\n",
    "data_input_ohe= data_input_ohe.drop('label_results', axis = 1)\n",
    "variables_lista = list(data_input_ohe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos TimeSeriesSplit para obtener las matrices de entrenamiento y prueba\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "for train_index, test_index in tscv.split(data_input_ohe):\n",
    "    X_train, X_test = data_input_ohe.iloc[train_index, :], data_input_ohe.iloc[test_index,:]\n",
    "    y_train, y_test = etiqueta.iloc[train_index], etiqueta.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata de las matrices para el modelo\n",
    "nrows_train = X_train.shape[0]\n",
    "nrows_test = X_test.shape[0]\n",
    "meta_train = pd.DataFrame({'nrows_train' : nrows_train,\n",
    "                           'nrows_test' : nrows_test}, index = [0])\n",
    "meta_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos un solo DF para los Datasets de Entrenamiento y Prueba y con la etiqueta\n",
    "X_train_1 = X_train.assign(Set = 'entrenamiento')\n",
    "X_train_1 = X_train_1.assign(etiqueta = y_train)\n",
    "X_test_1 = X_test.assign(Set = 'prueba')\n",
    "X_test_1 = X_test_1.assign(etiqueta = y_test)\n",
    "df_train_test = pd.concat([X_train_1, X_test_1], axis = 0)\n",
    "\n",
    "df_train_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(df_train_test.etiqueta) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(df_train_test,open(\"df_trn_tst.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones para regresar el DataFrame con las etiquetas y los sets de entrenamiento y \n",
    "# y prueba a los cuatro DF Xtrain, Ytrain, Xtst y Ytest\n",
    "X_train_2 = df_train_test[df_train_test.Set == 'entrenamiento']\n",
    "y_train_2 = X_train_2.etiqueta\n",
    "X_train_2 = X_train_2.iloc[:,0:df_train_test.shape[1]-2]\n",
    "\n",
    "\n",
    "X_test_2 = df_train_test[df_train_test.Set == 'prueba']\n",
    "y_test_2 = X_test_2.etiqueta\n",
    "X_test_2 = X_test_2.iloc[:,0:df_train_test.shape[1]-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmos a evaluar: DecisionTree y RandomForest\n",
    "algorithms_dict = {'tree': 'tree_grid_search'}\n",
    "algorithms = ['tree']\n",
    "# Hiperparámetros a evaluar en cada algoritmo:\n",
    "grid_search_dict = {'tree_grid_search': {'max_depth': [5,10,15], \n",
    "                                         'min_samples_leaf': [3,5,7]}}\n",
    "\n",
    "# Configuraciones generales de cada algoritmo a evaluar:\n",
    "estimators_dict = {'tree': DecisionTreeClassifier(random_state=1111)}\n",
    "best_estimators = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for algorithm in algorithms:\n",
    "    estimator = estimators_dict[algorithm]\n",
    "    grid_search_to_look = algorithms_dict[algorithm]\n",
    "    grid_params = grid_search_dict[grid_search_to_look]\n",
    "    gs = GridSearchCV(estimator, grid_params, scoring='precision', cv=tscv, n_jobs=-1)\n",
    "    start_time = time.time()\n",
    "    #train\n",
    "    gs.fit(X_train, y_train)\n",
    "    #best estimator\n",
    "    best_estimators.append(gs)\n",
    "time_exec = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tree = best_estimators[0].best_estimator_\n",
    "best_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.DataFrame(best_estimators[0].cv_results_)\n",
    "r = r.sort_values(\"rank_test_score\")\n",
    "r.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista = r.params.astype(str)\n",
    "test_mod = \"|\".join(lista)\n",
    "lista_2 = r.mean_test_score.astype(str)\n",
    "mean_scores = \"|\".join(lista_2)\n",
    "lista_3 = r.rank_test_score.astype(str)\n",
    "rank_model = \"|\".join(lista_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla = pd.DataFrame({\"best\":best_tree,\"modelos\":test_mod,\"mean_scores\":mean_scores,\n",
    "                      \"rank_model\":rank_model},index=[0])\n",
    "tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model.fit(X_train, y_train)\n",
    "pickle.dump(m, open(\"best_model.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = pickle.load(open(\"best_model.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados = m.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(resultados)\n",
    "res=res.rename(columns = {0:'label'})\n",
    "res.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_2 = pd.DataFrame(y_test)\n",
    "res_2.label_results.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels, metrics, n_groups, n_attribute, prop_pos_pred, prop_pos_real = bias_fair(m,df_trn_tst,df_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_pos_pred = df_labels.predicted.sum()/len(df_labels.predicted)\n",
    "prop_pos_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_pos_real = df_labels.label.sum()/len(df_labels.label)\n",
    "prop_pos_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento\n",
    "X_train = df_trn_tst[df_trn_tst.Set == 'entrenamiento']\n",
    "y_train = X_train.etiqueta\n",
    "X_train = X_train.iloc[:,0:df_trn_tst.shape[1]-2]\n",
    "# Prueba\n",
    "X_test = df_trn_tst[df_trn_tst.Set == 'prueba']\n",
    "y_test = X_test.etiqueta\n",
    "X_test = X_test.iloc[:,0:df_trn_tst.shape[1]-2]\n",
    "predicted_scores = m.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummy = pd.DataFrame()\n",
    "df_dummy['scores'] = pd.Series(predicted_scores[:,1])\n",
    "df_dummy['predic'] = np.where(df_dummy['scores'] < 0.7,0,1)\n",
    "df_dummy.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aeq2 = pd.DataFrame()\n",
    "df_aeq2['real'] = y_test\n",
    "df_aeq2['prediccion'] = df_dummy.predic\n",
    "df_aeq2['zip'] = df_fe['level'].tail(len(df_dummy.predic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aeq2 = df_aeq2.reset_index(drop=True)\n",
    "df_aeq2.columns = ['label_value','score','level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = Group()\n",
    "xtab2, attrbs2 = g2.get_crosstabs(df_aeq2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_metrics2 = g2.list_absolute_metrics(xtab2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtab2[[col for col in xtab2.columns if col not in absolute_metrics2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics2 = xtab2[['attribute_name', 'attribute_value']+[col for col in xtab2.columns if col in absolute_metrics2]].round(2)\n",
    "metrics2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = pd.concat([metrics2,metrics2]).reset_index(drop = True)\n",
    "con.attribute_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aeq2 = Plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_2 = aeq2.plot_group_metric(xtab2, 'for')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall2 = aeq2.plot_group_metric(xtab2, 'tpr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Cuáles son los atributos protegidos?**\n",
    "\n",
    "Se hicieron dos ejercicios. El primero consistió en tomar como atributo protegido el tipo de establecimiento (facility_type). Para generar esta variable, dado que existen 500 tipos de establecimiento, se clasificaron los grupos de mayor representación en la base de datos, resultando 4 los de mayor aparición (\"restaurant\",\"school\", \"grocery store\", \"children's services facility\"), después se agruparon tres categorías que se refieren a \"day care\" y el resto se clasificó como \"other\", esto por la heterogeneidad de establecimientos que ya no entraban en las categorías previas.\n",
    "\n",
    "El segundo consistió en tomar como atributo protegido el código postal (zip), para ello se creó una tabla de códigos postales clasificados por tipo de ingreso: \"High\", \"Low-mid\" y \"Down town\" (ver referencias) y, debido a que hay algunos códigos postales que se encuentran fuera del área de Chicago y no contamos con su clasificación de ingreso, se dejó en una categoría denominada \"other\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué grupos de referencia tiene cada atributo protegido?, explica el por qué**\n",
    "\n",
    "Para el atributo \"facility_type\", el grupo de referencia es la categoría de \"restaurante\", porque es la que tiene mayor representación en la base de datos y el objetivo sería que no haya sesgo en las predicciones con etiqueta negativa hacia este tipo de establecimiento.\n",
    "\n",
    "Para el atributo \"zip\", el grupo de referencia es la categoría de \"low-mid\" pues el objetivo sería que no haya sesgo en las predicciones con etiqueta negativa hacia este tipo de zonas (con menor ingreso), que pudieran generar mayor disparidad respecto a las demás zonas (\"High\" y \"Downtown\") tomando en cuenta que la cancelación de licencias de restaurantes puede afectar sensiblemente la economía o el desarrollo de alguna zona."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Tu modelo es punitivo o asistivo? explica por qué**\n",
    "\n",
    "De acuerdo con la pregunta análitica que se desea responder con el modelo predictivo (¿El establecimiento pasará o no la inspección?) y tomando en cuenta que el producto de datos estará orientado para que el uso sea por parte de los establecimientos y no del gobierno de Chicago para enviar inspecciones, consideramos que el modelo es asistivo ya que permitirá a los dueños de los establecimientos realizar consultas sobre si su establecimiento pasaría o no una inspección, permitiéndoles prevenir posibles multas o cancelaciones de licencia por incumplimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Qué métricas cuantificarás y ocuparás en sesgo e inequidad? explica por qué**\n",
    "\n",
    "- ***Equal Parity***: Esta métrica no es tan útil para nuestro objetivo porque nos interesa mejorar ninguno de los grupos, solo nos interesa orientar a los usuarios sobre si su establecimiento está o no en riesgo de pasar una posible inspección.\n",
    "\n",
    "- ***False Positive Parity***: \n",
    "\n",
    "\n",
    "- ***FNR Parity***: la interpretamos de la siguiente manera; dado que se recibe una alerta falsa, cuál es la probabilidad de que hayamos enviado una ambulancia dado la delegación a la que pertenece la llamada. Seleccionamos la métrica porque queremos verificar si alguna delegación tiene ventaja sobre otra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmos a evaluar: DecisionTree y RandomForest\n",
    "algorithms_dict = {'random_forest': 'rf_grid_search'}\n",
    "algorithms = ['random_forest']\n",
    "# Hiperparámetros a evaluar en cada algoritmo:\n",
    "grid_search_dict = {'rf_grid_search': {'n_estimators': [500],  \n",
    "                                      'max_depth': [5,10], \n",
    "                                      'min_samples_leaf': [10]}}\n",
    "\n",
    "# Configuraciones generales de cada algoritmo a evaluar:\n",
    "estimators_dict = {'random_forest': RandomForestClassifier(oob_score=True, random_state=2222)}\n",
    "best_estimators = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for algorithm in algorithms:\n",
    "    estimator = estimators_dict[algorithm]\n",
    "    grid_search_to_look = algorithms_dict[algorithm]\n",
    "    grid_params = grid_search_dict[grid_search_to_look]\n",
    "    gs = GridSearchCV(estimator, grid_params, scoring='precision', cv=tscv, n_jobs=-1)\n",
    "    start_time = time.time()\n",
    "    #train\n",
    "    gs.fit(X_train, y_train)\n",
    "    #best estimator\n",
    "    best_estimators.append(gs)\n",
    "time_exec = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = best_estimators[0].best_estimator_\n",
    "best_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = best_rf\n",
    "m_rf = model_rf.fit(X_train, y_train)\n",
    "pickle.dump(m, open(\"best_model_rf.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_rf = best_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_rf = pd.DataFrame(resultados_rf)\n",
    "res_rf = res_rf.rename(columns = {0:'label'})\n",
    "res_rf.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = pickle.load(open(\"consecutiva.pkl\",\"rb\"))\n",
    "type(cons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
